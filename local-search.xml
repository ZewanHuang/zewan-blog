<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Unity Inspector 自定义按钮</title>
    <link href="/2021/08/11/unity/editor-inspector/"/>
    <url>/2021/08/11/unity/editor-inspector/</url>
    
    <content type="html"><![CDATA[<p>在开发 Unity 项目时，为了有更快更方便的工作流，我们通常会在 Editor 下开发一些方便实用的工具。在工具中，最常用的就是自定义按钮。通过按钮，我们在不启动项目的情况下就能运行指定代码块。</p><p>此处以点击按钮生成立方块 Cube 的功能为例。</p><h2 id="点击按钮生成-Cube"><a href="#点击按钮生成-Cube" class="headerlink" title="点击按钮生成 Cube"></a>点击按钮生成 Cube</h2><p>在 Unity Inspector 创建自定义按钮，需要用到 Editor 和 MonoBehaviour 的继承类。</p><p>你可以按照我这样的项目框架来创建代码文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">Assets<br>└─Scripts<br>    └─InspectorButton<br>        ├──Editor<br>        │   └─CubeGeneratorEditor.cs<br>        └─CubeGenerator.cs<br></code></pre></td></tr></table></figure><p>我们只需要编写两个代码文件 <code>CubeGeneratorEditor.cs</code> 和 <code>CubeGenerator.cs</code></p><h3 id="MonoBehaviour-Script"><a href="#MonoBehaviour-Script" class="headerlink" title="MonoBehaviour Script"></a>MonoBehaviour Script</h3><p>继承 MonoBehaviour 的类，可作为 Component 挂载在 Unity 对象上。</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">using</span> UnityEngine;<br><br><span class="hljs-keyword">namespace</span> <span class="hljs-title">InspectorButton</span><br>&#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">CubeGenerator</span> : <span class="hljs-title">MonoBehaviour</span><br>    &#123;<br>        <span class="hljs-keyword">public</span> Vector3 CubePosition;<br><br>        <span class="hljs-comment">// 此处定义按钮函数供 Editor 按钮调用</span><br>        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">BuildCube</span>(<span class="hljs-params"></span>) <span class="hljs-comment">// 该函数创建 Cube 对象并将其加入挂载物体的子对象中</span></span><br>        &#123;<br>            GameObject cube = GameObject.CreatePrimitive(PrimitiveType.Cube);<br>            cube.transform.position = CubePosition;<br>            cube.transform.rotation = Quaternion.identity;<br>            <br>            cube.transform.SetParent(transform);<br>        &#125;<br>        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">DeleteAllCubes</span>(<span class="hljs-params"></span>) <span class="hljs-comment">// 该函数删除挂载物体的所有子对象</span></span><br>        &#123;<br>            <span class="hljs-built_in">int</span> childCount = transform.childCount;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; childCount ; i++) &#123;<br>                DestroyImmediate(transform.GetChild (<span class="hljs-number">0</span>).gameObject);<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Editor-Script"><a href="#Editor-Script" class="headerlink" title="Editor Script"></a>Editor Script</h3><p><strong>继承 Editor 的类，必须放在 Editor 文件夹下。</strong>我们在该类中定义 Inspector 按钮，并调用上面代码定义的函数。</p><p>因为我声明了命名空间 <code>InspectorButton.Editor</code>，因此必须指明该类继承于 <code>UnityEditor.Editor</code>。当然，声明命名空间并不必要，不声明时继承略写为 <code>Editor</code> 即可。</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-keyword">using</span> UnityEditor;<br><span class="hljs-keyword">using</span> UnityEngine;<br><br><span class="hljs-keyword">namespace</span> <span class="hljs-title">InspectorButton.Editor</span><br>&#123;<br>    [<span class="hljs-meta">CustomEditor(typeof(CubeGenerator))</span>]<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">CubeGeneratorEditor</span> : <span class="hljs-title">UnityEditor.Editor</span><br>    &#123;<br>        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">override</span> <span class="hljs-keyword">void</span> <span class="hljs-title">OnInspectorGUI</span>(<span class="hljs-params"></span>)</span><br>        &#123;<br>            DrawDefaultInspector(); <span class="hljs-comment">// 默认 Inspector 样式</span><br>            CubeGenerator script = (CubeGenerator) target;<br><br>            <span class="hljs-comment">// 此处定义按钮调用 CubeGenerator 的函数</span><br>            <span class="hljs-keyword">if</span> (GUILayout.Button(<span class="hljs-string">&quot;Generate Cube&quot;</span>))<br>            &#123;<br>                script.BuildCube();<br>            &#125;<br>            <span class="hljs-keyword">if</span> (GUILayout.Button(<span class="hljs-string">&quot;Delete All Cubes&quot;</span>))<br>            &#123;<br>                script.DeleteAllCubes();<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="创建对象挂载脚本"><a href="#创建对象挂载脚本" class="headerlink" title="创建对象挂载脚本"></a>创建对象挂载脚本</h3><p>在 Unity 中创建一个名为 <code>Cubes</code> 的空对象，点击 <code>Add Component</code> 挂载 <code>CubeGenerator</code> 脚本。</p><p>填入 <code>Cube Position</code>，点击 <code>Generate Cube</code> 即可生成 Cube 对象，点击 <code>Delete All Cubes</code> 即可删除前面生成的所有 Cube 对象。</p><p><img src="/img/articles/21-8-12/01.png"></p><h2 id="点击按钮创建人物角色"><a href="#点击按钮创建人物角色" class="headerlink" title="点击按钮创建人物角色"></a>点击按钮创建人物角色</h2><p>上面使用生成 Cube 对象的例子，介绍了创建 Inspector 自定义按钮的方法。</p><p>我还在 Invector 第三方角色控制器的基础上，编写了简易的角色创建按钮 (Character Creator) 和活动角色管理按钮 (Active Charater Manager)，可供参考。</p><p>相关 Unitypackage 链接为 <a href="https://github.com/ZewanHuang/unitypackages/tree/master/ThirdController_adapted_from_Invector_on_creator">github.com/ZewanHuang/unitypackages</a></p><blockquote><p>我会在该仓库中更新实用插件的 Unity Packages，欢迎 Star ~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Unity</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Unity</tag>
      
      <tag>Inspector</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络表述</title>
    <link href="/2021/07/27/ml/neural-networks/"/>
    <url>/2021/07/27/ml/neural-networks/</url>
    
    <content type="html"><![CDATA[<p>在 <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8E%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8F/">机器学习『吴恩达』</a> 系列中，无论是线性回归还是逻辑回归，都有一个缺点：当特征太多时，计算的负荷非常大。</p><p>而事实上，我们的训练集常常有太多的特征。比如，我们试图训练一个模型来识别视觉对象（比如识别是否为汽车），一种方法是，我们利用图片上一个个像素的值来作为特征，更基础的我们假设只选用灰度图片，这样每个像素只有一个值（而非 RGB 值），且假设采用的都是 $50\times 50$ 像素的小图片，这样就会有 $2500$ 个特征；如果进一步两两特征组合成非线性模型，则大概有 $2500^{2}/2$ 个特征。</p><p>普通的逻辑回归模型，不能有效地处理这么多特征，这时候我们需要<strong>神经网络</strong>。</p><h2 id="神经元和大脑"><a href="#神经元和大脑" class="headerlink" title="神经元和大脑"></a>神经元和大脑</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>神经网络产生的目的是，人们想尝试设计出模仿大脑的算法。</p><p>大脑是十分复杂的，我们能学习数学、编程等多种东西，你可能会想象到，如果想模仿大脑，可能需要写很多不同的算法、软件来模拟所有这些五花八门的学习事件。但能不能假设，大脑做这些事情，不需要过多不同的程序去实现。相反的，<strong>大脑处理事件的方法，只需要一个单一的学习算法就可以。</strong></p><p><img src="/img/articles/21-7-27/01.jpg" alt="图源于Coursera"></p><p>当然，这只是个假设，不过目前还是有一些相关证据的。</p><p>神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。结果表明听觉皮层学会了“看”。这类实验称为<strong>神经重接实验</strong>。</p><p>从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。</p><p>这些只作为前言唠一唠，我们还是主要介绍神经网络的技术细节。</p><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>为了构建神经网络模型，首先我们需要思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元/神经核（<strong>processing unit/Nucleus</strong>），它含有许多输入/树突（<strong>input/Dendrite</strong>），并且有一个输出/轴突（<strong>output/Axon</strong>）。神经网络是大量神经元相互连接并通过电脉冲来交流的一个网络。</p><p><img src="/img/articles/21-7-27/02.jpg" alt="图源于Coursera"></p><p>神经元的作用在于，把自己收到的消息进行计算，并向其它神经元传递消息。比如，想活动一块肌肉，会触发一个神经元向肌肉发送消息，再引起肌肉收缩。我们后面建立的神经网络模型，也是以此为基础的。</p><h2 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h2><h3 id="单实例解释"><a href="#单实例解释" class="headerlink" title="单实例解释"></a>单实例解释</h3><p>神经网络模型建立在很多神经元上，每一个神经元又是学习模型。这些神经元（也叫<strong>激活单元</strong>，<strong>activation unit</strong>），采纳一些特征作为输入，并根据自身模型提供输出。</p><p>暂且不理会中间的处理，只看输入输出，我们可以简单地了解神经网络：</p><p>$$<br>[x_{0}\ x_{1}\ x_{2}\ \cdots]\ \rightarrow \ [\ ]\  \rightarrow \ h_{\theta}(x)<br>$$</p><p>以<strong>逻辑回归模型</strong>为神经元示例，设计如下神经网络：</p><p><img src="/img/articles/21-7-27/03.png" alt="图源于Coursera"></p><p>图中的激活单元都是一个逻辑回归模型，其中：</p><ul><li>输入单元：$x_{1},x_{2},x_{3}$，我们将原始数据输入给它们</li><li>中间单元：处理数据，并呈递给下一层</li><li>输出单元：计算 $h_{\theta}(x)$</li></ul><p>神经网络模型是许多逻辑单元按照不同层次组织起来的网络，每一层的输出变量都是下一层的输入变量。如下图，第一层为<strong>输入层</strong> (<strong>Input Layer</strong>)，中间层为<strong>隐藏层</strong> (<strong>Hidden Layers</strong>)，最后一层为<strong>输出层</strong> (<strong>Output Layer</strong>)。另外，我们为每一层增加一个<strong>偏差单元</strong> (<strong>bias unit</strong>)。</p><p>偏差单元有点像我们之前在讲述线性回归和逻辑回归时，为满足矩阵运算而添加的单元特征。</p><p><img src="/img/articles/21-7-27/04.png"></p><p>我们用符号来描述模型：</p><p>$$<br>\begin{align*}&amp; a_i^{(j)} = \text{“activation” of unit $i$ in layer $j$} \newline&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}\end{align*}<br>$$</p><p>$a_{i}^{(j)}$ 代表第 $j$ 层的第 $i$ 个激活单元，$\Theta^{(j)}$ 代表从第 $j$ 层到第 $j+1$ 层的权重矩阵（参数矩阵），其尺寸为：以第 $j+1$ 层激活单元数量为行数，以第 $j$ 层激活单元数为列数。例如，上图的神经网络 $\Theta^{(1)}$ 为 $3\times 4$ 矩阵。</p><p>对于上图的模型，计算表达式为：</p><p>$$<br>\begin{align*} a_1^{(2)} &amp;= g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} &amp;= g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} &amp;= g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) &amp;= a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}<br>$$</p><p>上面的表达仅仅将特征矩阵的一行（一个训练实例）feed 给了神经网络，我们需要将整个训练集都喂给算法进行学习。</p><h3 id="向量化表示"><a href="#向量化表示" class="headerlink" title="向量化表示"></a>向量化表示</h3><p>从前面的介绍，我们知道，中间层每一个 $a$ 都是由上一层所有的 $x$ 和每个 $x$ 对应的权重所决定的。我们把这样<strong>从左到右</strong>的算法称为<strong>前向传播算法</strong> (<strong>Forward Propagation</strong>)。</p><p>仍然使用上面的例子，先对一个训练实例，将计算过程展开：</p><p>首先我们有输入值 $x=[x_{0}\ x_{1}\ x_{2}\ x_{3}]^{T}$，使用 $z^{(2)}=\Theta^{(1)}x,a^{(2)}=g(z^{(2)})$ 计算第二层的值：</p><p>$$<br>g(<br>    \left[<br>        \begin{matrix}<br>        \theta_{10}^{(1)} &amp; \theta_{11}^{(1)} &amp; \theta_{12}^{(1)} &amp; \theta_{13}^{(1)}\newline<br>        \theta_{20}^{(1)} &amp; \theta_{21}^{(1)} &amp; \theta_{22}^{(1)} &amp; \theta_{23}^{(1)}\newline<br>        \theta_{30}^{(1)} &amp; \theta_{31}^{(1)} &amp; \theta_{32}^{(1)} &amp; \theta_{33}^{(1)}<br>        \end{matrix}<br>    \right]<br>    \times<br>    \left[<br>        \begin{matrix}<br>        x_{0}\newline<br>        x_{1}\newline<br>        x_{2}\newline<br>        x_{3}\newline<br>        \end{matrix}<br>    \right]<br>)=g(<br>    \left[<br>        \begin{matrix}<br>        \theta_{10}^{(1)}x_{0}+\theta_{11}^{(1)}x_{1}+\theta_{12}^{(1)}x_{2}+\theta_{13}^{(1)}x_{3}\newline<br>        \theta_{20}^{(1)}x_{0}+\theta_{21}^{(1)}x_{1}+\theta_{22}^{(1)}x_{2}+\theta_{23}^{(1)}x_{3}\newline<br>        \theta_{30}^{(1)}x_{0}+\theta_{31}^{(1)}x_{1}+\theta_{32}^{(1)}x_{2}+\theta_{33}^{(1)}x_{3}\newline<br>        \end{matrix}<br>    \right]<br>)=\left[<br>    \begin{matrix}<br>        a_{1}^{(2)}\newline<br>        a_{2}^{(2)}\newline<br>        a_{3}^{(2)}\newline<br>    \end{matrix}<br>\right]<br>$$</p><p>计算后给第二层添加 $a_{0}^{(2)}=1$，计算输出的值为：</p><p>$$<br>g(<br>    \left[<br>        \begin{matrix}<br>        \theta_{10}^{(2)} &amp; \theta_{11}^{(2)} &amp; \theta_{12}^{(2)} &amp; \theta_{13}^{(2)}<br>        \end{matrix}<br>    \right]<br>    \times<br>    \left[<br>        \begin{matrix}<br>        a_{0}^{(2)} \newline<br>        a_{1}^{(2)} \newline<br>        a_{2}^{(2)} \newline<br>        a_{3}^{(2)} \newline<br>        \end{matrix}<br>    \right]<br>)=g(\theta_{10}^{(2)}a_{0}^{(2)}+\theta_{11}^{(2)}a_{1}^{(2)}+\theta_{12}^{(2)}a_{2}^{(2)}+\theta_{13}^{(2)}a_{3}^{(2)})=h_{\theta}(x)<br>$$</p><p>这样我们展示了单个训练实例的计算过程，接下来我们用向量化表示整个训练集的计算。</p><p>我们需要对训练集特征矩阵进行转置，使同一个实例的特征都在同一列中。即：</p><p>$$<br>a^{(2)} = g(\Theta^{(1)} \times X^{T})<br>$$</p><p>事实上中间层可能不止一个，我们可以归纳出计算公式：</p><p>$$<br>a^{(j+1)} = g(\Theta^{(j)} \times a^{(j)})<br>$$</p><p>从上面的过程我们可以看出，神经网络就像是 logistic regression，只不过把特征值不断地进行计算和变换，从而将原始输入值变为高级的特征值，可以把 $a_{1},a_{2},a_{3}$ 理解成 $x_{0},x_{1},x_{2},x_{3}$ 的进化体。因为过程是梯度下降的，所以中间量 $a$ 变得越来越厉害，所以这些更高级的特征值远比 $x$ 次方更厉害，能更好地预测新数据。<strong>这就是神经网络相比于逻辑回归和线性回归的优势。</strong></p><h2 id="应用举例"><a href="#应用举例" class="headerlink" title="应用举例"></a>应用举例</h2><h3 id="实现逻辑运算"><a href="#实现逻辑运算" class="headerlink" title="实现逻辑运算"></a>实现逻辑运算</h3><p>接下来，我们构建神经网络，来实现基础的逻辑运算，包括逻辑与、或、非。</p><p>我们可以用这样的一个神经网络来表示 <strong>AND</strong> 函数：</p><p><img src="/img/articles/21-7-27/05.png" alt="图源于Coursera"></p><p>其中权重为 $[-30\ 20\ 20]$，即 $\theta_{0}=-30,\theta_{1}=20,\theta_{2}=20$，则输出函数为 $h_{\Theta}(x)=g(-30+20x_{1}+20x_{2})$</p><p>而我们知道 $g(x)$ 的图像是：</p><p><img src="/img/articles/21-7-27/06.png" alt="图源于Coursera"></p><p>这样我们赋值 $x_{1},x_{2}\in \{0,1\}$，得到：</p><p>$$<br>\begin{align*}&amp; x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline &amp; x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1\end{align*}<br>$$</p><p>这样，我们有 $h_{\Theta}(x)=x_{1} \text{AND} x_{2}$</p><p>从上面逻辑与神经网络的构建过程，我们可以看到，我们只需搭建一个无隐藏层的神经网络即可实现，只需在权重上合理分配，即可达到效果。后面将直接给出其它两个逻辑运算的神经网络。</p><ul><li><strong>逻辑与 AND</strong>：权重为 $-30,20,20$</li><li><strong>逻辑或 OR</strong>：权重为 $-10,20,20$</li><li><strong>逻辑非 NOT</strong>：权重为 $10,-20$</li></ul><p><img src="/img/articles/21-7-27/07.png" alt="AND &amp; OR &amp; NOT"></p><p>我们还可以将神经元组合成更复杂的神经网络，以实现更复杂的运算。例如实现 <strong>XNOR</strong> 功能，要求当且仅当输入的两个值相等时输出 1，反之则输出 0，即</p><p>$$<br>\text{XNOR} = (x_{1}\ \text{AND}\ x_{2})\ \text{OR}\ ((\text{NOT}\ x_{1})\ \text{AND}\ (\text{NOT} \ x_{2}))<br>$$</p><p><img src="/img/articles/21-7-27/08.png" alt="XNOR"></p><p>这样我们就得到了一个能实现 <strong>XNOR</strong> 运算符的神经网络。我们来检验一下。</p><table><thead><tr><th align="center">$x_{1}$</th><th align="center">$x_{2}$</th><th align="center">$a_{1}^{(2)}$</th><th align="center">$a_{2}^{(2)}$</th><th align="center">$h_{\Theta}(x)$</th></tr></thead><tbody><tr><td align="center">$0$</td><td align="center">$0$</td><td align="center">$0$</td><td align="center">$1$</td><td align="center">$1$</td></tr><tr><td align="center">$0$</td><td align="center">$1$</td><td align="center">$0$</td><td align="center">$0$</td><td align="center">$0$</td></tr><tr><td align="center">$1$</td><td align="center">$0$</td><td align="center">$0$</td><td align="center">$0$</td><td align="center">$0$</td></tr><tr><td align="center">$1$</td><td align="center">$1$</td><td align="center">$1$</td><td align="center">$0$</td><td align="center">$1$</td></tr></tbody></table><p>按这种组合神经元的方法，我们可以逐渐构造出越来越复杂的函数，也能得到更加厉害的特征，这正是神经网络的优势。</p><h3 id="多类别分类"><a href="#多类别分类" class="headerlink" title="多类别分类"></a>多类别分类</h3><p>前面我们介绍过逻辑回归解决多类别分类问题的方法，神经网络同样能实现。</p><p>如果我们要训练一个神经网络来识别路人、汽车、摩托车和卡车，在输出层我们应有 4 个值，分别对应以上四种类别。</p><p><img src="/img/articles/21-7-27/09.png" alt="图源于Coursera"></p><p>我们定义结果如下，对应路人、汽车、摩托车和卡车：</p><p>$$<br>y^{(i)}=<br>\left[<br>\begin{matrix}<br>1\newline 0\newline 0\newline 0<br>\end{matrix}<br>\right],<br>\left[<br>\begin{matrix}<br>0\newline 1\newline 0\newline 0<br>\end{matrix}<br>\right],<br>\left[<br>\begin{matrix}<br>0\newline 0\newline 1\newline 0<br>\end{matrix}<br>\right],<br>\left[<br>\begin{matrix}<br>0\newline 0\newline 0\newline 1<br>\end{matrix}<br>\right]<br>$$</p><p>这样我们构建出来的神经网络大概如下：</p><p>$$<br>\left[\begin{matrix}x_{0}\newline x_{1}\newline x_{2}\newline \cdots\newline x_{n}\end{matrix}\right]<br>\rightarrow<br>\left[\begin{matrix}a_{0}^{(2)}\newline a_{1}^{(2)}\newline a_{2}^{(2)}\newline \cdots \end{matrix}\right]<br>\rightarrow<br>\left[\begin{matrix}a_{0}^{(3)}\newline a_{1}^{(3)}\newline a_{2}^{(3)}\newline \cdots \end{matrix}\right]<br>\rightarrow \cdots\rightarrow<br>\left[\begin{matrix}h_{\Theta}(x)_{1}\newline h_{\Theta}(x)_{2}\newline h_{\Theta}(x)_{3}\newline h_{\Theta}(x)_{4}\newline\end{matrix}\right]<br>$$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从大脑和神经元的人体结构引入，介绍了神经网络的结构，由输入层、隐藏层、输出层组成。同时我们以逻辑回归模型建立激活单元，表示出神经网络，并给出计算公式：$a^{(j+1)} = g(\Theta^{(j)} \times a^{(j)})$</p><p>最后我们构造神经网络模型，建立了逻辑运算和多类别分类的模型。当然，本文并没有给出参数求解的细节，而是给出神经网络的表述，帮助理解神经网络的结构和概念。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.coursera.org/learn/machine-learning">[1] 吴恩达 Andrew Ng 机器学习课程</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">[2] 黄海广博士的机器学习笔记</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习『吴恩达』</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>过拟合与正则化</title>
    <link href="/2021/07/26/ml/overfit/"/>
    <url>/2021/07/26/ml/overfit/</url>
    
    <content type="html"><![CDATA[<p>在 <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8E%E5%90%B4%E6%81%A9%E8%BE%BE%E3%80%8F/">机器学习『吴恩达』</a> 系列中，我们已经介绍了几种不同的学习算法，包括线性回归和逻辑回归，能够有效地解决许多问题。但是应用时，会遇到过拟合 <strong>(over-fitting)</strong> 的问题，可能会导致效果较差。</p><h2 id="过拟合-over-fitting"><a href="#过拟合-over-fitting" class="headerlink" title="过拟合 (over-fitting)"></a>过拟合 (over-fitting)</h2><p>所以什么是过拟合？我们从下面这个回归问题的例子来认识过拟合。</p><p><img src="/img/articles/21-7-26/fit.jpg" alt="图源于Coursera"></p><ul><li>第一个模型是一个线性模型，欠拟合，不能很好地适应训练集；</li><li>第二个模型较好地拟合了数据，并贴合数据的变化趋势；</li><li>第三个模型是一个四次方模型，过于强调拟合原始数据，而丢失了数据的发展趋势。</li></ul><p>可以看到，第三个模型很好地拟合了数据，但丢失了算法的本质——预测新数据。向第三个模型中输入一个新的值使之预测，它将表现得很差，因为它丢失了数据的发展趋势，失去规律。这就是<strong>过拟合</strong>问题。</p><div class="note note-info">            <p><strong>过拟合</strong>（<strong>overfitting</strong>，或称拟合过度），是指过于紧密或精确地匹配特定数据集，以致于无法良好地拟合其他数据或预测未来的观察结果的现象。（<a href="https://zh.wikipedia.org/wiki/%E9%81%8E%E9%81%A9">维基百科</a>）</p>          </div><p>分类问题中也存在这样的问题：</p><p><img src="/img/articles/21-7-26/classification-fit.jpg" alt="图源于Coursera"></p><p>单纯以多项式模型来理解，$x$ 的次数越高，拟合得越好，但相应的预测能力就可能变差。</p><p>针对过拟合问题，处理的方式有：</p><ol><li>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA）</li><li><strong>正则化</strong>。保留所有的特征，但是减小参数 (<strong>magnitude</strong>)</li></ol><p>后面我们主要介绍正则化的方法，在这里简单解释一下上面第一个方法的可行性。</p><p><strong>过拟合通常是发生在特征较多的情况下的。</strong>试想如果特征少，我们用简单的低次模型就可以实现较好的拟合效果，且低次常常能维护正常的数据发展趋势。因此针对过拟合问题，我们可以丢弃低效特征，减少特征数量，简化模型，达到拟合度和预测能力的平衡。</p><h2 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 (Regularization)"></a>正则化 (Regularization)</h2><h3 id="修改代价函数"><a href="#修改代价函数" class="headerlink" title="修改代价函数"></a>修改代价函数</h3><p>如果我们的线性模型 $h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^{2}+\theta_{3}x_{3}^{3}+\theta_{4}x_{4}^{4}$ 出现了过拟合现象，我们要做的就是在一定程度上减小参数 $\theta$ 的值，这就是<strong>正则化的基本方法</strong>。</p><p>我们知道，过拟合是因为高次项，因此我们可以决定减小 $\theta_{3},\theta_{4}$ 的大小，要做的是修改代价函数，在 $\theta_{3},\theta_{4}$ 上设置一些惩罚。这样，我们在最小化代价时，会将该惩罚考虑进去，从而选择较小的 $\theta_{3},\theta_{4}$。</p><p>修改后的代价函数如：</p><p>$$<br>\mathbf{\min}_{\theta}\ J(\theta) = \frac{1}{2m}\sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^{2} + 1000\cdot \theta_{3}^{2}+1000\cdot \theta_{4}^{2}<br>$$</p><p>假如我们有非常多的特征，我们不确定要惩罚哪些特征，这时我们可以对所有特征进行惩罚，并让代价函数优化软件来选择惩罚的程度。这样我们得到了新的代价函数：</p><p>$$<br>\mathbf{\min}_{\theta}\ J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^{2} + \lambda \sum_{j=1}^{n}\theta_{j}^{2}]<br>$$</p><p>其中，$\lambda$ 称为正则化参数 (<strong>Regularization Parameter</strong>)。根据惯例，我们不对 $\theta_{0}$ 进行惩罚。</p><p>经过正则化处理的模型与原模型的可能对比图如下：</p><p><img src="/img/articles/21-7-26/result.jpg" alt="图源于Coursera"></p><p>如果 $\lambda$ 过大，会把所有参数最小化，导致模型趋于 $h_{\theta}(x)=\theta_{0}$，这样得到的可能是一条平行于 $x$ 轴的直线，造成欠拟合。</p><h3 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h3><p>对线性回归的求解，我们在<a href="/2021/07/12/ml-regression/">『线性回归』</a>一文中推导了两种学习算法：分别基于梯度下降和正规方程。</p><h4 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 (Gradient Descent)"></a>梯度下降 (Gradient Descent)</h4><p>我们将正则化的代价函数应用到梯度下降中，因为我们未对 $\theta_{0}$ 进行正则化，所以分两种情形更新参数：</p><p>$$<br>\mathbf{Repeat}\ \{<br>\begin{aligned}<br>\theta_{0} : &amp;= \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{0}^{(i)}\\<br>\theta_{j} : &amp;= \theta_{j} - \alpha [ \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} + \frac{\lambda}{m} \theta_{j} ], \ j\in \{1,2,\cdots,n\}<br>\end{aligned}<br>\}<br>$$</p><p>对上面算法中 $j=1,2,\cdots,n$ 的情况进行调整得：</p><p>$$<br>\theta_{j} := \theta_{j} (1-\alpha \frac{\lambda}{m}) - \alpha\frac{1}{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)}<br>$$</p><p>可以看出，正则化线性回归的梯度下降算法的变化在于，每次参数更新都减小一个额外的值。</p><h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p>我们同样可以利用正规方程求解正则化线性回归模型，向量化公式如下：</p><p>$$<br>\theta = (X^{T}X+\lambda\cdot L)^{-1} X^{T}y<br>$$</p><p>其中 $L$ 为 $n+1$ 维方阵：</p><p>$$<br>L =<br>\left[<br>\begin{matrix}<br>0 \\<br>\  &amp; 1  &amp;    &amp;    &amp; \\<br>\  &amp; \  &amp; 1  &amp;    &amp; \\<br>\  &amp; \  &amp; \  &amp; \ddots &amp; \\<br>\  &amp; \  &amp; \  &amp; \  &amp; 1 \\<br>\end{matrix}<br>\right]<br>$$</p><h3 id="正则化逻辑回归"><a href="#正则化逻辑回归" class="headerlink" title="正则化逻辑回归"></a>正则化逻辑回归</h3><p>我们同样可以对逻辑回归的代价函数正则化：</p><p>$$<br>J(\theta) =<br>\frac{1}{m} \sum_{i=1}^{m}<br>[-y^{(i)}\log(h_{\theta}(x^{(i)})) - (1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_{j}^{2}<br>$$</p><p><strong>Python 代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">costReg</span>(<span class="hljs-params">theta, X, y, learningRate</span>):</span><br>    theta = np.matrix(theta)<br>    X = np.matrix(X)<br>    y = np.matrix(y)<br>    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))<br>    second = np.multiply((<span class="hljs-number">1</span> - y), np.log(<span class="hljs-number">1</span> - sigmoid(X*theta.T)))<br>    reg = (learningRate / (<span class="hljs-number">2</span> * <span class="hljs-built_in">len</span>(X))* np.<span class="hljs-built_in">sum</span>(np.power(theta[:,<span class="hljs-number">1</span>:theta.shape[<span class="hljs-number">1</span>]],<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(first - second) / (<span class="hljs-built_in">len</span>(X)) + reg<br></code></pre></td></tr></table></figure><p>使用梯度下降算法求解，最终得到的参数更新公式与线性回归一致：</p><p>$$<br>\mathbf{Repeat}\ \{<br>\begin{aligned}<br>\theta_{0} : &amp;= \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{0}^{(i)}\\<br>\theta_{j} : &amp;= \theta_{j} - \alpha [ \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} + \frac{\lambda}{m} \theta_{j} ], \ j\in \{1,2,\cdots,n\}<br>\end{aligned}<br>\}<br>$$</p><p>但仍需注意，看上去和线性回归一样，但实际上 $h_{\theta}$ 是不同的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了过拟合问题——过于拟合数据集而丢失预测能力，并介绍正则化方法，主要通过修改代价函数，对指定或全部参数进行惩罚，从而达到减小参数的效果。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.coursera.org/learn/machine-learning">[1] 吴恩达 Andrew Ng 机器学习课程</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">[2] 黄海广博士的机器学习笔记</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习『吴恩达』</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分类问题与逻辑回归</title>
    <link href="/2021/07/23/ml/classification/"/>
    <url>/2021/07/23/ml/classification/</url>
    
    <content type="html"><![CDATA[<p>本文主要介绍使用逻辑回归算法解决分类问题，包括定义其模型、代价函数，并使用梯度下降算法解决问题。</p><h2 id="分类问题与表示-Classification"><a href="#分类问题与表示-Classification" class="headerlink" title="分类问题与表示 (Classification)"></a>分类问题与表示 (Classification)</h2><p>在<a href="https://blog.zewan.cc/2021/07/10/ml-welcome/">『机器学习的基础分类与概念』</a>一文中，我们介绍了监督学习中的分类问题。</p><p>在分类问题中，我们尝试预测的是数据是否属于某一个类，比如：判断一封电子邮件是否是垃圾邮件，判断一次金融交易是否为欺诈，区别肿瘤是恶性还是良性。</p><p>从简单的二元分类问题开始讨论。</p><p>二元分类指的是，因变量 (dependent variable) 可能属于的类有且仅有两个，分别称之为负向类 (<strong>negative class</strong>) 和正向类 (<strong>positive class</strong>)。我们定义因变量 $y\in {0,1}$，其中 $0$ 表示负向类，$1$ 表示正向类。</p><p><img src="/img/articles/21-7-24/linear.png"></p><p>我们先试图用一下线性回归算法来解决二元分类问题（当然这不是一个好算法）。如上图，我们可以看到，定义一个线性回归模型来拟合分类数据时，函数的输出值可能既不等于 1 也不等于 0，此时可假设：</p><ul><li>$h_{\theta}(x)\geq 0.5$ 时，预测值 $y=1$</li><li>$h_{\theta}(x)\lt 0.5$ 时，预测值 $y=0$</li></ul><p>当然这里所定义的线性模型 $h_{\theta}(x)$ 是不合理的，它的输出值可能远大于 1 或远小于 0，这都不是我们想要的。下面我们将使用<strong>逻辑回归</strong>定义分类问题。</p><h3 id="假设表示-Hypothesis-Representation"><a href="#假设表示-Hypothesis-Representation" class="headerlink" title="假设表示 (Hypothesis Representation)"></a>假设表示 (Hypothesis Representation)</h3><p>前面采用线性回归模型，因其预测值可能不在 $[0,1]$ 范围内，不适合解决分类问题。</p><p>我们引入逻辑回归，使模型输出值范围始终在 0 和 1 之间。该模型假设为：</p><p>$$<br>h_{\theta}(x)=g(\theta^{T}x)<br>$$</p><p>其中 $x$ 表示样本特征值，$g$ 表示逻辑函数 (<strong>logistic function</strong>)，此处使用的是常用的逻辑函数 <strong>Sigmoid function</strong>：</p><p>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">z</span>):</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br></code></pre></td></tr></table></figure><p>该函数图像为：</p><p><img src="/img/articles/21-7-24/sigmoid.png" alt="图源于Coursera"></p><p>如上图所示，函数 $g(z)$ 将实数范围内的值映射到区间 $[0,1]$ 内，从而更符合分类问题的特征。</p><p>事实上，$h_{\theta}(x)$ 展示的是输出值为 1 的可能性 (<strong>probability</strong>)，例如 $h_{\theta}(x)=0.7$ 表示输出值为 1 的几率为 70%。</p><p>$$<br>h_{\theta}(x)=P(y=1|x;\theta)=1-P(y=0|x;\theta)<br>$$</p><h3 id="决策边界-Decision-Boundary"><a href="#决策边界-Decision-Boundary" class="headerlink" title="决策边界 (Decision Boundary)"></a>决策边界 (Decision Boundary)</h3><p>决策边界的概念能帮助理解逻辑回归的假设函数在计算什么。</p><p>在逻辑回归中，我们预测：</p><ul><li>当 $h_{\theta}(x)\geq 0.5$ 时，预测 $y=1$</li><li>当 $h_{\theta}(x)\lt 0.5$ 时，预测 $y=0$</li></ul><p>根据前面所绘制的 Sigmoid 函数图像，我们得到：</p><ul><li>$z=0$ 时 $g(z)=0.5$</li><li>$z&gt;0$ 时 $g(z)&gt;0.5$</li><li>$z&lt;0$ 时 $g(z)&lt;0.5$</li></ul><p>又 $z=\theta^{T}x$，则：$\theta^{T}x\geq 0$ 时 $y=1$；$\theta^{T}x&lt;0$ 时 $y=0$</p><p>此处，<strong>决策边界指划分 $y=0$ 和 $y=1$ 两个区域的边界线，该边界由假设函数决定。</strong></p><p>延续上面所述，假设参数值为 $\theta^{T}=[\begin{matrix} 5&amp;-1&amp;0 \end{matrix}]$，则可求出边界值：$y=1\ if \ 5+(-1)x_{1}+0 x_{2}\geq 0$，得 $x_{1}\leq 5$。在这个例子中，决策边界为 $x_{1}=5$ 的线，在其左边输出值为 $y=1$，位于其右边的点输出值为 $y=0$。</p><p><img src="/img/articles/21-7-24/ex1.png"></p><p>当然，<strong>决策边界不一定是一条直线</strong>。当数据呈下图分布时，需要使用曲线才能分隔 $y=0$ 和 $y=1$ 的区域，即定义高次方函数来表示模型，比如 $h_{\theta}(x)=g(\theta_{0} + \theta_{1} x_{1} + \theta_{2} x_{2} + \theta_{3} x_{1}^{2} + \theta_{4} x_{2}^{2})$，当给定一组参数值时，令 $g(z)$ 中 $z=0$ 可求得决策边界。</p><p><img src="/img/articles/21-7-24/ex2.png"></p><h2 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h2><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 (Cost Function)"></a>代价函数 (Cost Function)</h3><p>前面我们介绍了分类问题的模型表示，接下来定义代价函数，以使用梯度下降算法进行求解。</p><p>我们不能够使用与线性回归相同的代价函数来定义逻辑回归模型，因为逻辑回归模型的假设为 $h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}x}}$，将其带入线性回归的代价函数后，我们得到的是一个非凸函数 (non-convex)，将导致有许多局部最小值，影响梯度下降算法的使用。</p><p><img src="/img/articles/21-7-24/non-convex.jpg" alt="图源于Coursera"></p><p>因此，我们重新定义逻辑回归的代价函数：</p><p>$$<br>J(\theta) = \frac{1}{m} \sum_{i=1}^{m} Cost(h_{\theta} (x^{(i)}), y^{(i)})<br>$$</p><p>其中</p><p>$$<br>Cost(h_{\theta}(x), y) =<br>\begin{cases}<br>-log(h_{\theta}(x)) &amp; if\ y=1 \\<br>-log(1-h_{\theta}(x)) &amp; if\ y=0<br>\end{cases}<br>$$</p><p>$h_{\theta}(x)$ 与 $Cost$ 之间的关系如下图所示：</p><p><img src="/img/articles/21-7-24/cost.jpg" alt="图源于Coursera"></p><p>可以看到：</p><ul><li>当实际值 $y=1$ 时，$h_{\theta}(x)$ 也为 1 时误差为 0，且越远离 1 误差增大；</li><li>当实际值 $y=0$ 时，$h_{\theta}(x)$ 也为 0 时误差为 0，且越远离 0 误差增大。</li></ul><p>因此，总体上，该代价函数是符合要求的。我们进一步简化代价函数：</p><p>$$<br>Cost(h_{\theta}(x),y)=-y\times log(h_{\theta}(x)) - (1-y)\times log(1-h_{\theta}(x))<br>$$</p><p>带入代价函数中得到：</p><p>$$<br>J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [ y^{(i)}log(h_{\theta} (x^{(i)})) + (1-y^{(i)}) log(1-h_{\theta}(x^{(i)})) ]<br>$$</p><p>上面定义的逻辑回归的代价函数，我们所选的函数 $J(\theta)$ 是一个<strong>凸函数，没有局部最优值</strong>。它是一个凸优化问题，这是可证的，以后再补上证明过程。</p><p>使用向量化表示：</p><p>$$<br>J(\theta) = \frac{1}{m} \cdot (-y^{T} log(h) - (1-y)^{T} log(1-h))<br>$$</p><p>其中，$h=g(X\theta^{T})$，$X$ 代表特征矩阵。</p><p>Python 代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost</span>(<span class="hljs-params">theta, X, y</span>):</span><br>    theta = np.matrix(theta)<br>    X = np.matrix(X)<br>    y = np.matrix(y)<br>    first = np.multiply(-y, np.log(sigmoid(X* theta.T)))<br>    second = np.multiply((<span class="hljs-number">1</span> - y), np.log(<span class="hljs-number">1</span> - sigmoid(X* theta.T)))<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(first - second) / (<span class="hljs-built_in">len</span>(X))<br></code></pre></td></tr></table></figure><h3 id="梯度下降求解-Gradient-Descent"><a href="#梯度下降求解-Gradient-Descent" class="headerlink" title="梯度下降求解 (Gradient Descent)"></a>梯度下降求解 (Gradient Descent)</h3><p>上面我们定义了逻辑回归的代价函数，接下来使用梯度下降算法求解。</p><p>回顾一下梯度下降算法：</p><p>$$<br>Repeat\ \{<br>\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}}J(\theta)<br>\}<br>$$</p><p>将逻辑回归的代价函数 $J(\theta)$ 带入计算，可获得：</p><blockquote><p>这里再省略一波计算过程，后面补上</p></blockquote><p>$$<br>Repeat\ \{<br>\theta_{j}:=\theta_{j}-\frac{\alpha}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}<br>\}<br>$$</p><p><strong>注意，尽管它和线性回归的式子长得一样，但假设的定义发生了变化。</strong></p><ul><li>线性回归假设：$h_{\theta}(x)=\theta^{T}x=\theta_{0}x_{0}+\theta_{1}x_{1}+\cdots+\theta_{n}x_{n}$</li><li>逻辑回归假设：$h_{\theta}(x)=g(\theta^{T}x)=\frac{1}{1+e^{-\theta^{T}x}}$</li></ul><p>因此，即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。</p><p>更新参数的规则，我们同样使用向量化表示：</p><p>$$<br>\theta := \theta - \frac{\alpha}{m} X^{T} (g(X\theta^{T}) - y)<br>$$</p><p>这样我们就了解了如何实现逻辑回归，包括其假设、代价函数、参数更新的计算等等。</p><h2 id="多类别分类-Multiclass-Classification"><a href="#多类别分类-Multiclass-Classification" class="headerlink" title="多类别分类 (Multiclass Classification)"></a>多类别分类 (Multiclass Classification)</h2><h3 id="举例介绍一对多"><a href="#举例介绍一对多" class="headerlink" title="举例介绍一对多"></a>举例介绍一对多</h3><p>前面所讲述的都是二元分类问题 (Binary classification)，现在我们来说一说如何使用逻辑回归解决多类别分类问题，用的是<strong>一对多 (one-vs-all)</strong> 的分类算法。</p><p>先举几个多类别分类的例子。</p><ol><li>给邮件归类，区分开工作邮件、来自朋友的邮件、来自家人的邮件，这样类别有四个，可以用 $y=1,y=2,y=3,y=4$ 表示；</li><li>病情诊断，鼻塞的症状，可能没生病，用 $y=1$ 表示，有可能患感冒 $y=2$、得流感 $y=3$；</li><li>天气的分类问题，区分哪些天是晴天、多云、雨天或下雪天……</li></ol><p><img src="/img/articles/21-7-24/binary-multiple.png" alt="图源于Coursera"></p><p>之前的二元分类问题，数据可能如上面的左图；对于一个多类别分类问题，数据集大概跟右图类似。</p><p>我们已经知道如何进行二元分类，使用逻辑回归，将数据集一分为二为正向类和负向类。下面将介绍如何使用<strong>一对多</strong>的分类思想，解决多类别分类问题。</p><p><img src="/img/articles/21-7-24/one-vs-all.png" alt="图源于Coursera"></p><p>假设我们有这样的一个训练集，它有三个类别，用三角形、矩形、叉号表示，分别对应 $y=1$、$y=2$、$y=3$。我们从类别一开始，创建一个新的“伪”训练集，<strong>将类别1定义为正向类，类别2和3定为负向类</strong>。这样我们就创建出了一个新的训练集，如上图所示，我们要拟合出这个新的分类器。</p><p>上面这样转变后，问题就变成了我们前面所介绍的二元分类问题了。定义模型 $h_{\theta}^{(1)}(x)$，它其实就对应我们前面所说的假设函数，<strong>表示属于类别1的可能性大小</strong>。使用逻辑回归我们可能获得参数最优解，进而解得该模型。</p><p>接着，类似地我们选择类别2记为正向类，其它类别1和3为负向类，定义模型为 $h_{\theta}^{(2)}(x)$；选择类别3记为正向类，其它类别1和2为负向类，定义模型为 $h_{\theta}^{(3)}(x)$。最后我们得到 3 个模型。</p><p>最后，在我们做预测时，对于每一个输入变量，都需要将这 3 个分类器运行一遍，选择最高可能性的类别作为输出值。比如输入 $x$，计算得到 $h_{\theta}^{(1)}(x)&gt;h_{\theta}^{(2)}(x)&gt;h_{\theta}^{(3)}(x)$，我们就说它属于类别1。</p><h3 id="一对多小结"><a href="#一对多小结" class="headerlink" title="一对多小结"></a>一对多小结</h3><p>通过前面的例子，大概了解了多类别分类的思路，这里再小结一下。</p><p>假设我们拿到的是含 $k$ 个类别的训练集，我们要把它<strong>转换为 $k$ 个二元分类器</strong>。</p><p>将多个类中的一个类标记为正向类 ($y=1$)，其它类记为负向类，该模型记作 $h_{\theta}^{(1)}(x)$；接着，选择第二个类作为正向类 ($y=2$)，其它类为负向类，记作 $h_{\theta}^{(2)}(x)$，依此类推。</p><p>最后得到一系列模型，简记为 </p><p>$$<br>h_{\theta}^{(i)}(x)=P(y=i|x;\theta)<br>$$</p><p>其中 $i=1,2,\cdots,k$</p><p>在需要做预测时，我们输入新值 $x$，在所有 ($k$ 个) 分类器中都运行一遍，选择使 $h_{\theta}^{(i)}(x)$ 值最大的 $i$，即 $\max\limits_{i}h_{\theta}^{(i)}(x)$，作为输出值。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文我们首先介绍了二元分类问题的表示和求解：</p><ol><li>模型假设：$h_{\theta}(x)=g(\theta^{T}x)$，其中 $g(z)=\frac{1}{1+e^{-z}}$</li><li>代价函数：$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [ y^{(i)}log(h_{\theta} (x^{(i)})) + (1-y^{(i)}) log(1-h_{\theta}(x^{(i)})) ]$</li><li>梯度下降求解：$Repeat\ \{<br>\theta_{j}:=\theta_{j}-\frac{\alpha}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}<br>\}$</li></ol><p>接着介绍如何求解多类别分类问题：将含 n 个类别的训练集，转换为 n 个二元分类器，选取可能性最大的类别作为结果。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.coursera.org/learn/machine-learning">[1] 吴恩达 Andrew Ng 机器学习课程</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">[2] 黄海广博士的机器学习笔记</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习『吴恩达』</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>资源信息库</title>
    <link href="/2021/07/22/notes/materials/"/>
    <url>/2021/07/22/notes/materials/</url>
    
    <content type="html"><![CDATA[<h2 id="Research"><a href="#Research" class="headerlink" title="Research"></a>Research</h2><ul><li><a href="/sources/RE00-How-to-Write-a-SIGGRAPH-Paper.pdf">How to write a SIGGRAPH paper</a>：SIGGRAPH Course 教你怎么写 SIGGRAPH 论文</li><li><a href="/sources/RE01-How-to-do-research.pdf">How to do research</a>：MIT 教授 Bill Freeman 告诉你如何做研究</li><li><a href="/sources/RE02-Elements-of-a-successful-graduate-career.pdf">Elements of a successful graduate career</a>：MIT 教授 Bill Freeman 找众多教授收集的成功秘诀</li><li><a href="/sources/RE03-Notes-on-writing.pdf">Notes on writing</a>：MIT 教授 Fredo Durand 的写论文秘诀</li><li><a href="/sources/RE04-Graduate-student-survival-guide.pdf">Graduate student survival guide</a>：Waterloo 大学教授撰写的研究生生存指南</li></ul><h2 id="UX-AR"><a href="#UX-AR" class="headerlink" title="UX/AR"></a>UX/AR</h2><ul><li><a href="https://www.zhihu.com/question/474370179/answer/2040838303">AR 了解与入门 - 知乎问题</a></li><li><a href="https://zhuanlan.zhihu.com/c_1236335814964682752">隐形的界面：万物互联时代的交互设计 - 知乎专栏</a></li></ul><h3 id="系统教程"><a href="#系统教程" class="headerlink" title="系统教程"></a>系统教程</h3><ul><li><a href="https://blog.csdn.net/yolon3000/category_9023477.html">ARFoundation 之路 - DavidWang</a></li><li><a href="https://blog.csdn.net/yolon3000/category_9283315.html">ARCore 之路 - DavidWang</a></li><li><a href="https://blog.csdn.net/yolon3000/category_10141425.html">ARKit 之路 - DavidWang</a></li><li><a href="https://blog.csdn.net/yolon3000/category_10976435.html">HoloLens2 之路 - DavidWang</a></li></ul><h2 id="Unity"><a href="#Unity" class="headerlink" title="Unity"></a>Unity</h2><h3 id="Unity-Animator"><a href="#Unity-Animator" class="headerlink" title="Unity/Animator"></a>Unity/Animator</h3><ul><li><a href="https://blog.csdn.net/qq_28849871/article/details/72593569">Unity/Animation – 创建Animation Clip</a></li><li><a href="http://blog.csdn.net/qq_28849871/article/details/72771779">Unity/Animation – 调节Animation Curves</a></li><li><a href="http://blog.csdn.net/qq_28849871/article/details/72821633">Unity/Animation – 添加动画事件(Animation Events)</a></li><li><a href="https://blog.csdn.net/qq_28849871/article/details/77914922">Unity/Animation – 创建Animator Controller</a></li></ul><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="系统学习"><a href="#系统学习" class="headerlink" title="系统学习"></a>系统学习</h3><ul><li><a href="http://www.ai-start.com/ml2014/">斯坦福大学吴恩达2014机器学习教程中文笔记目录 - 黄海广博士译</a></li><li><a href="https://www.coursera.org/learn/machine-learning">机器学习 Coursera 网课 - Andrew Ng 吴恩达</a></li></ul><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><h3 id="系统学习-1"><a href="#系统学习-1" class="headerlink" title="系统学习"></a>系统学习</h3><ul><li><a href="https://www.coursera.org/specializations/deep-learning">深度学习专项课程 Coursera 网课 - Andrew Ng 吴恩达</a></li></ul><h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><ul><li><a href="https://www.zhihu.com/column/c_145288300">直观梳理深度学习 - 大佬专栏</a></li><li><a href="https://zhuanlan.zhihu.com/p/31561570">深度学习基础(基本概念、优化算法、初始化、正则化等)</a></li><li><a href="https://zhuanlan.zhihu.com/p/31727402">计算机视觉四大基本任务(分类、定位、检测、分割)</a></li><li><a href="https://zhuanlan.zhihu.com/p/31727405">计算机视觉其他应用(网络压缩、视觉问答、可视化、风格迁移等)</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>矩阵求导</title>
    <link href="/2021/07/18/math/matrix-derivation/"/>
    <url>/2021/07/18/math/matrix-derivation/</url>
    
    <content type="html"><![CDATA[<p>本篇主要介绍向量、矩阵求导的定义，包含向量、矩阵、标量之间的求导，并给出一些常用的方法和恒等式。</p><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><table><thead><tr><th align="center">符号</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">$\mathbf{A},\mathbf{X},\mathbf{Y}$</td><td align="center"><strong>矩阵</strong></td></tr><tr><td align="center">$\mathbf{a},\mathbf{x},\mathbf{y}$</td><td align="center"><strong>向量</strong></td></tr><tr><td align="center">$a,x,y$</td><td align="center"><strong>标量</strong></td></tr><tr><td align="center">$X^{T}$</td><td align="center">矩阵 $X$ 的转置</td></tr><tr><td align="center">$tr(X)$</td><td align="center">矩阵 $X$ 的迹</td></tr><tr><td align="center">$det(X)$ 或 $|X|$</td><td align="center">矩阵 $X$ 的行列式</td></tr><tr><td align="center">$||X||$</td><td align="center">矩阵 $X$ 的范数</td></tr></tbody></table><h2 id="矩阵向量求导定义"><a href="#矩阵向量求导定义" class="headerlink" title="矩阵向量求导定义"></a>矩阵向量求导定义</h2><h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p>在高等数学中，我们学过标量对标量的求导，比如标量 $y$ 对标量 $x$ 的求导，可表示为 $\frac{\partial y}{\partial x}$。</p><p>有时我们会有一组标量 $y_{i},i=1,2,…$，对标量 $x$ 求导，我们会得到一组结果：</p><p>$$<br>\frac{\partial y_{i}}{\partial x},i=1,2,…<br>$$</p><p>如果将该组标量 $y_{i}$ 组成向量 $\mathbf{y}$，我们得到的计算结果也会是一个向量 $\frac{\partial \mathbf{y}}{\partial x}$。</p><p>向量对标量的求导，正是<strong>将向量的每个分量分别对标量求导</strong>，最后组成一个向量的形式。</p><p>根据求导的自变量和因变量的类型，属于标量、向量或矩阵，我们有 9 种可能的矩阵求导定义：</p><table><thead><tr><th align="center">自变量\因变量</th><th align="center">标量 $y$</th><th align="center">向量 $\mathbf{y}$</th><th align="center">矩阵 $\mathbf{Y}$</th></tr></thead><tbody><tr><td align="center">标量 $x$</td><td align="center">$\frac{\partial y}{\partial x}$</td><td align="center">$\frac{\partial \mathbf{y}}{\partial x}$</td><td align="center">$\frac{\partial \mathbf{Y}}{\partial x}$</td></tr><tr><td align="center">向量 $\mathbf{x}$</td><td align="center">$\frac{\partial y}{\partial \mathbf{x}}$</td><td align="center">$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$</td><td align="center">$\frac{\partial \mathbf{Y}}{\partial \mathbf{x}}$</td></tr><tr><td align="center">矩阵 $\mathbf{X}$</td><td align="center">$\frac{\partial y}{\partial \mathbf{X}}$</td><td align="center">$\frac{\partial \mathbf{y}}{\partial \mathbf{X}}$</td><td align="center">$\frac{\partial \mathbf{Y}}{\partial \mathbf{X}}$</td></tr></tbody></table><p>事实上，矩阵、向量、标量之间的求导，是将因变量各个元素分别对自变量各个元素进行求导。比如向量 $\mathbf{y}$ 对向量 $\mathbf{x}$ 求导，则分别将  $\mathbf{y}$ 的每个分量对 $\mathbf{x}$ 的每个分量求导。</p><p>但这样自然会出现一个排列次序的问题。我们将两个长度为 $m,n$ 的向量的分量对应求导，得到 $m\times n$ 个标量。为简练表示结果，我们需要将其写成矩阵的形式，这样就涉及到这 $m\times n$ 个值如何排列。</p><p>为解决这个问题，需要引入<strong>求导布局</strong>的概念。</p><h3 id="求导布局"><a href="#求导布局" class="headerlink" title="求导布局"></a>求导布局</h3><p>为解决矩阵向量求导的结果不唯一，我们引入求导布局。最基本的求导布局有两个：分子布局 (numerator layout) 和分母布局 (denominator layout)。</p><p>对于<strong>分子布局</strong>而言，求导结果的维度以分子为主。也就是说，如果向量 $\mathbf{y}$ 是一个 $m$ 维的<u>列向量</u>，它对标量 $x$ 的求导结果 $\frac{\partial \mathbf{y}}{\partial x}$ 也是一个 $m$ 维<u>列向量</u>；若 $\mathbf{y}$ 是一个 $m$ 维的<u>行向量</u>，则求导结果也是一个 $m$ 维的<u>行向量</u>。</p><p>对于<strong>分母布局</strong>而言，求导结果的维度以分母为主。也就是说，如果向量 $\mathbf{y}$ 是一个 $m$ 维的<u>列向量</u>，它对标量 $x$ 的求导结果 $\frac{\partial \mathbf{y}}{\partial x}$ 是一个 $m$ 维<u>行向量</u>；若 $\mathbf{y}$ 是一个 $m$ 维的<u>行向量</u>，则求导结果也是一个 $m$ 维的<u>列向量</u>。</p><p>可以看出，分子布局和分母布局相差一个转置。</p><p>稍微麻烦些的是向量对向量的求导。比如 $m$ 维列向量 $\mathbf{y}$ 对 $n$ 维列向量 $\mathbf{x}$ 求导，如果是分子布局，结果为：</p><p>$$<br>\frac{\partial \mathbf{y}}{\partial \mathbf{x}}<br>=<br>\left(<br>\begin{matrix}<br>\frac{\partial y_{1}}{\partial x_{1}} &amp; \frac{\partial y_{1}}{\partial x_{2}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}  \\<br>\frac{\partial y_{2}}{\partial x_{1}} &amp; \frac{\partial y_{2}}{\partial x_{2}} &amp; \cdots &amp; \frac{\partial y_{2}}{\partial x_{n}}  \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>\frac{\partial y_{m}}{\partial x_{1}} &amp; \frac{\partial y_{m}}{\partial x_{2}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>\end{matrix}<br>\right)<br>$$</p><p>上边这个按分子布局求得的向量对向量的求导结果，一般叫做<strong>雅克比 (Jacobian) 矩阵</strong>，有的资料又表示为 $\frac{\partial \mathbf{y}}{\partial \mathbf{x}^{T}}$。</p><p>如果按分母布局，则结果为 $n\times m$ 的矩阵：</p><p>$$<br>\frac{\partial \mathbf{y}}{\partial \mathbf{x}}<br>=<br>\left(<br>\begin{matrix}<br>\frac{\partial y_{1}}{\partial x_{1}} &amp; \frac{\partial y_{2}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}  \\<br>\frac{\partial y_{1}}{\partial x_{2}} &amp; \frac{\partial y_{2}}{\partial x_{2}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{2}}  \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>\frac{\partial y_{1}}{\partial x_{n}} &amp; \frac{\partial y_{2}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>\end{matrix}<br>\right)<br>$$</p><p>上边这个按分子布局求得的向量对向量的求导结果，一般叫做<strong>梯度矩阵</strong>，有的资料又表示为 $\frac{\partial \mathbf{y}^{T}}{\partial \mathbf{x}}$。</p><p>观察上面两个矩阵，我们可以发现，分子布局的结果矩阵，保留了原分子向量的维度，即结果矩阵的每一列都是 $y_{1}$ 至 $y_{m}$，行数仍为 $m$；分母布局保留了原分母向量的维度，即结果矩阵的每一列都是 $x_{1}$ 至 $x_{n}$，行数为 $n$。</p><p>显然，<strong>对于某一种求导类型，不能同时使用分子布局和分母布局求导。</strong></p><p>在机器学习中，我们使用的是<strong>混合布局</strong>的思路。如果是向量或矩阵对标量求导，则使用分子布局；如果是标量对向量、矩阵求导，则以分母布局为准；而向量对向量求导，有些分歧，本文以分子布局的雅可比矩阵为主。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><table><thead><tr><th align="center">自变量\因变量</th><th>标量 $y$</th><th>列向量 $\mathbf{y}$</th><th>矩阵 $\mathbf{Y}$</th></tr></thead><tbody><tr><td align="center">标量 $x$</td><td>$\frac{\partial y}{\partial x}$</td><td><strong>分子布局：</strong>$m$ 维列向量<br>分母布局：$m$ 维行向量</td><td><strong>分子布局：</strong>$m\times n$<br>分母布局：$n\times m$</td></tr><tr><td align="center">列向量 $\mathbf{x}$</td><td>分子布局：$n$ 维行向量<br><strong>分母布局：</strong>$n$ 维列向量</td><td><strong>分子布局：</strong>$m\times n$ 雅可比矩阵<br>分母布局：$n\times m$ 梯度矩阵</td><td></td></tr><tr><td align="center">矩阵 $\mathbf{X}$</td><td>分子布局：$n\times m$<br><strong>分母布局：</strong>$m\times n$</td><td></td><td></td></tr></tbody></table><h2 id="定义法求导"><a href="#定义法求导" class="headerlink" title="定义法求导"></a>定义法求导</h2><h3 id="标量对向量求导"><a href="#标量对向量求导" class="headerlink" title="标量对向量求导"></a>标量对向量求导</h3><p>标量对向量求导，严格来说是实值函数对向量求导。即定义实值函数 $f:R^{n}\rightarrow R$，自变量 $\mathbf{x}$ 是 $n$ 维向量，输出值 $y$ 是标量。</p><p>前面我们也说过，标量对向量求导，实际上就是<strong>对向量的每个分量分别求导</strong>，再把结果排列在一起。那么我们可以将实值函数对向量的每一个分量求导，找到规律，得到结果向量。</p><p>举个简单的例子：$y=\mathbf{a}^{T}\mathbf{x}$，求解 $\frac{\partial\mathbf{a}^{T}\mathbf{x}}{\partial\mathbf{x}}$</p><p>根据定义，对 $\mathbf{x}$ 的第 $i$ 个分量求导：</p><p>$$<br>\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial x_{i}}<br>=<br>\frac{\partial \sum_{j=1}^{n} a_{j} x_{j}}{\partial x_{i}}<br>=<br>\frac{\partial a_{i} x_{i}}{\partial x_{i}}<br>=<br>a_{i}<br>$$</p><p>可见，对向量的第 $i$ 个分量的求导结果等于向量 $\mathbf{a}$ 的第 $i$ 个分量。采用分母布局，最后得到的求导结果为 $\mathbf{a}$。即：</p><p>$$<br>\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial \mathbf{x}}<br>=<br>\mathbf{a}\tag{1}<br>$$</p><p>同样使用定义法，我们可以得到</p><p>$$<br>\frac{\partial \mathbf{x}^{T} \mathbf{a}}{\partial \mathbf{x}}<br>=<br>\mathbf{a},<br>\frac{\partial \mathbf{x}^{T} \mathbf{x}}{\partial \mathbf{x}}<br>=<br>2\mathbf{x}\tag{2}<br>$$</p><p>再来看一个复杂点的例子：$y=\mathbf{x}^{T}\mathbf{A}\mathbf{x}$，求解 $\frac{\partial \mathbf{x}^{T} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}$</p><blockquote><p>不难推断矩阵 $\mathbf{A}$ 是方阵</p></blockquote><p>对 $\mathbf{x}$ 的第 $k$ 个分量进行求导如下：</p><p>$$<br>\frac{\partial \mathbf{x^{T}Ax}}{\partial x_{k}}<br>=<br>\frac{\partial \sum_{i=1}^{n} \sum_{j=1}^{n} x_{i}A_{ij}x_{j}}{\partial x_{k}}<br>=<br>\sum_{i=1}^{n}A_{ik}x_{i}<br>+<br>\sum_{j=1}^{n}A_{kj}x_{j}<br>$$</p><p>将这 $k$ 个分量组成向量，结果可表示为</p><p>$$<br>\frac{\partial \mathbf{x}^{T} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}<br>=<br>\mathbf{A}^{T}\mathbf{x}<br>+<br>\mathbf{Ax}\tag{3}<br>$$</p><h3 id="标量对向量求导的一些基本法则"><a href="#标量对向量求导的一些基本法则" class="headerlink" title="标量对向量求导的一些基本法则"></a>标量对向量求导的一些基本法则</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://en.wikipedia.org/wiki/Matrix_calculus">[1] Matrix Calculus - Wikipedia</a></p><p><a href="https://fei-wang.github.io/matrix.html">[2] 矩阵求导 - 蒙奇 D 路飞</a></p><p><a href="https://www.cnblogs.com/pinard/p/10750718.html">[3] 机器学习中的矩阵向量求导 - 刘建平Pinard</a></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>矩阵</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unity 使用 bvh 驱动骨骼动作</title>
    <link href="/2021/07/15/unity/unity-bvh/"/>
    <url>/2021/07/15/unity/unity-bvh/</url>
    
    <content type="html"><![CDATA[<p>本篇主要介绍如何在 Unity 中使用 bvh 驱动骨骼动画，同时从理论与实操两个方面进行阐述。</p><blockquote><p>代码已打包成 unitypackage，见链接 <a href="https://github.com/ZewanHuang/unitypackages/tree/master/3_BVHParser">BVHParser</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>先简单介绍一些相关的理论基础。</p><h3 id="BVH"><a href="#BVH" class="headerlink" title="BVH"></a>BVH</h3><p>BVH 文件是使用设备对人体运动进行捕获后产生的文件，它包含<strong>角色的骨骼和肢体关节旋转数据</strong>，是一种通用的人体特征动画文件格式。</p><p><img src="/img/articles/21-7-15/bvh.jpg" alt="图源于百度百科"></p><p>上图是 bvh 最常记录的骨骼点，图用节点表示关节，连线表示躯干，身体的各个部分形成子树的形式。</p><p>BVH 文件的第一部分定义了关节树、每个关节点的名称、关节与关节之间的相对位置（<strong>偏移量</strong>），即基本骨架。Hips 关节点作为整个人体的根节点，拥有三维空间位置参数，从而完成了对人体运动情况的完整描述。</p><p>BVH 文件的第二部分记录了运动的数据，定义了动作数据持续的长度（<strong>帧数</strong>）以及每帧之间的<strong>时间间隔</strong>。且按照第一部分定义的关节顺序提供每帧数据，记录了每一帧中各个关节点的位置信息和旋转信息（<strong>局部旋转量</strong>）。</p><p>BVH 文件示例见 <a href="https://raw.githubusercontent.com/ZewanHuang/BVHParser/master/Assets/Resources/bvh/13_29.bvh">Example.bvh</a></p><h3 id="角色姿势"><a href="#角色姿势" class="headerlink" title="角色姿势"></a>角色姿势</h3><p>一般来说，角色模型或 BVH 都有它的<strong>内置姿势</strong>，即创建模型时所设定的姿势。将一个模型<strong>所有关节的局部旋转量设置为单位四元数</strong>，则可显示出其内置姿势。</p><p>在对角色模型或 BVH 的处理中，通常涉及到三种姿势：A 型姿势(A-Pose)、<strong>T 型姿势</strong>(T-Pose)和其它姿势，其中 A-Pose 和 T-Pose 通常作为内置姿势或第一帧骨骼姿势。</p><p><img src="/img/articles/21-7-15/tpose.jpg" alt="T-Pose"></p><p><img src="/img/articles/21-7-15/apose.jpg" alt="A-Pose"></p><h2 id="驱动理论"><a href="#驱动理论" class="headerlink" title="驱动理论"></a>驱动理论</h2><p>了解了一些基础理论，接下来就来介绍一下 bvh 驱动的基本原理。</p><p>由于 bvh 的骨架与 unity 所使用和展示的骨架差异较大，因此仅赋值是无法实现需求的。但不管是 bvh 还是 unity 骨架，大多数都是基于 Tpose 的，且不同骨架的 Tpose 姿势一致，因此我们利用 Tpose 作为媒介，将 bvh 中的所有动画帧迁移至 unity 中。</p><p>接下来的推证，前提是 <strong>bvh 的第一帧是 Tpose</strong>。</p><h3 id="转换流程"><a href="#转换流程" class="headerlink" title="转换流程"></a>转换流程</h3><p><img src="/img/articles/21-7-15/flow.jpg" alt="转换流程图"></p><p>理解一下这个图。起初我们所拿到的数据是，Unity 模型的骨骼点信息和 BVH 每一帧动画关节点的局部旋转量；要实现的是，求出图中矩阵 $T_{5}$，应用到 <strong>Unity 每个关节的旋转量</strong>上，使 Unity 模型展示出相应动作。</p><p>事实上，将该图命名为流程图不太合适，可以理解成 $T_{5}$ 的<strong>求解过程</strong>吧。</p><p>先看 BVH。将 BVH 某一帧的所有关节点旋转量乘上矩阵 $T_{2}$，变换成 Tpose，再乘上 $T_{4}$，变化为相应动画。之所以这样做，是为了拿出 <strong>Tpose</strong> 这个中间媒介。Unity 和 BVH 骨架的 Tpose 姿势一致，想要展示的动作也一致，因此 $T_{4}$ 作用于 Unity 的 Tpose上，可以在 Unity 模型上展示出动作。</p><p>这样的话，求解 $T_{5}$ 仅需再求出 $T_{1}$ 即可实现，我们下面具体介绍一下求解原理。</p><h3 id="矩阵求解"><a href="#矩阵求解" class="headerlink" title="矩阵求解"></a>矩阵求解</h3><h4 id="变换矩阵-T-1-T-2"><a href="#变换矩阵-T-1-T-2" class="headerlink" title="变换矩阵 $T_{1}$/$T_{2}$"></a>变换矩阵 $T_{1}$/$T_{2}$</h4><p>通常而言，CMU 等提供的 BVH 动画，第一帧通常就是 Tpose，Unity 导入模型后通常也是 T 型姿势，因此无需复杂的转换。</p><p>但需要注意，我们所获得的数据信息，是骨骼节点相对于父节点的局部旋转量，需要使用下面公式将其转换为全局旋转矩阵 (Rotation)。</p><p>$$<br>R_{i}=R_{p}\times r_{i}<br>$$</p><p>其中，$R_{i}$ 为所求的当前节点的全局旋转（四元数），$R_{p}$ 为父节点的全局旋转，$r_{i}$ 为当前节点的局部旋转。</p><p>使用上式将 Unity 模型和 BVH <strong>第一帧</strong>的局部旋转转化为全局旋转，则得到了矩阵 $T_{1}$、$T_{2}$。</p><h4 id="变换矩阵-T-3"><a href="#变换矩阵-T-3" class="headerlink" title="变换矩阵 $T_{3}$"></a>变换矩阵 $T_{3}$</h4><p>BVH 中记录了完整的动画数据，我们可以根据它们计算出每一帧所有关节的坐标位置 (Position)。</p><p>在 BVH 中，根节点比其它节点多了个位置信息，根据根节点的位置信息 Root Position、关节层次关系 Hierachy、各节点相对父节点的偏移量 Offset (BVH 初始姿势)和每一帧节点旋转量，就可以推算出所有关节的坐标位置：</p><p>$$<br>Pos_{i} = Pos_{p} + R_{p} \times Offset_{i}<br>$$</p><p>其中，$Pos_{i}$ 为当前关节点坐标，$Pos_{p}$ 为父节点坐标，$R_{p}$ 为父节点全局旋转，$Offset_{i}$ 为当前节点相对父节点的偏移量 (Vector3)。</p><h4 id="变换矩阵-T-4"><a href="#变换矩阵-T-4" class="headerlink" title="变换矩阵 $T_{4}$"></a>变换矩阵 $T_{4}$</h4><p>前面我们说过，$T_{4}$ 是迁移的关键矩阵，计算出它，我们就能计算出 $T_{5}$。</p><p>观察流程图，我们可以推出：</p><p>$$<br>T_{2}\times T_{4} = T_{3}<br>$$</p><p>则</p><p>$$<br>T_{4} = T_{2}^{-1} \times T_{3}<br>$$</p><h4 id="变换矩阵-T-5"><a href="#变换矩阵-T-5" class="headerlink" title="变换矩阵 $T_{5}$"></a>变换矩阵 $T_{5}$</h4><p>Tpose 姿势一致，动画效果一致，因此 $T_{4}$ 可作用于 Unity 的 Tpose 上。故我们可求出 $T_{5}$：</p><p>$$<br>T_{5} = T_{1} \times T_{4} = T_{1} \times T_{2}^{-1} \times T_{3}<br>$$</p><h3 id="位置调整"><a href="#位置调整" class="headerlink" title="位置调整"></a>位置调整</h3><p>上述矩阵都是作用于各关节点的旋转量上的，但动画除此之外还有<strong>根节点的位置</strong>，通过调整它来调整人物的位置。</p><p>因此 BVH 的人物大小和 Unity 模型大小不同，所以我们通常根据某根骨骼的长度计算缩放比例，然后对 BVH 的根节点位置乘以缩放比例，就得到 Unity 根节点的位置了。</p><p>$$<br>Pos_{r}^{(unity)} = Pos_{r}^{(bvh)} \times Scale<br>$$</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>前面讲解了 BVH 驱动相关原理，接下来大致讲述一下核心代码实现。项目代码见 <a href="https://github.com/ZewanHuang/BVHParser">BVHParser</a>。</p><h3 id="核心代码"><a href="#核心代码" class="headerlink" title="核心代码"></a>核心代码</h3><p><strong>获取关节父子关系</strong></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs Csharp"><span class="hljs-function"><span class="hljs-keyword">public</span> Dictionary&lt;<span class="hljs-built_in">string</span>,<span class="hljs-built_in">string</span>&gt; <span class="hljs-title">getHierachy</span>(<span class="hljs-params"></span>)</span><br>&#123;<br>    Dictionary&lt;<span class="hljs-built_in">string</span>, <span class="hljs-built_in">string</span>&gt; hierachy = <span class="hljs-keyword">new</span> Dictionary&lt;<span class="hljs-built_in">string</span>, <span class="hljs-built_in">string</span>&gt;();<br>    <span class="hljs-keyword">foreach</span> (BVHBone bb <span class="hljs-keyword">in</span> boneList)<br>    &#123;<br>        <span class="hljs-keyword">foreach</span> (BVHBone bbc <span class="hljs-keyword">in</span> bb.children)<br>        &#123;<br>            hierachy.Add(bbc.name, bb.name);<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> hierachy;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>欧拉角转四元数</strong></p><p>注意所用的 bvh 数据是否是 <code>ZYX</code> 顺序，若不是，需要根据 bvh 的顺序修改函数参数顺序。</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs Csharp"><span class="hljs-function"><span class="hljs-keyword">private</span> Quaternion <span class="hljs-title">eul2quat</span>(<span class="hljs-params"><span class="hljs-built_in">float</span> z, <span class="hljs-built_in">float</span> y, <span class="hljs-built_in">float</span> x</span>)</span><br>&#123;<br>    z = z * Mathf.Deg2Rad;<br>    y = y * Mathf.Deg2Rad;<br>    x = x * Mathf.Deg2Rad;<br><br>    <span class="hljs-comment">// 动捕数据是ZYX，但是unity是ZXY</span><br>    <span class="hljs-built_in">float</span>[] c = <span class="hljs-keyword">new</span> <span class="hljs-built_in">float</span>[<span class="hljs-number">3</span>];<br>    <span class="hljs-built_in">float</span>[] s = <span class="hljs-keyword">new</span> <span class="hljs-built_in">float</span>[<span class="hljs-number">3</span>];<br>    c[<span class="hljs-number">0</span>] = Mathf.Cos(x / <span class="hljs-number">2.0f</span>); c[<span class="hljs-number">1</span>] = Mathf.Cos(y / <span class="hljs-number">2.0f</span>); c[<span class="hljs-number">2</span>] = Mathf.Cos(z / <span class="hljs-number">2.0f</span>);<br>    s[<span class="hljs-number">0</span>] = Mathf.Sin(x / <span class="hljs-number">2.0f</span>); s[<span class="hljs-number">1</span>] = Mathf.Sin(y / <span class="hljs-number">2.0f</span>); s[<span class="hljs-number">2</span>] = Mathf.Sin(z / <span class="hljs-number">2.0f</span>);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Quaternion(<br>        c[<span class="hljs-number">0</span>] * c[<span class="hljs-number">1</span>] * s[<span class="hljs-number">2</span>] - s[<span class="hljs-number">0</span>] * s[<span class="hljs-number">1</span>] * c[<span class="hljs-number">2</span>],<br>        c[<span class="hljs-number">0</span>] * s[<span class="hljs-number">1</span>] * c[<span class="hljs-number">2</span>] + s[<span class="hljs-number">0</span>] * c[<span class="hljs-number">1</span>] * s[<span class="hljs-number">2</span>],<br>        s[<span class="hljs-number">0</span>] * c[<span class="hljs-number">1</span>] * c[<span class="hljs-number">2</span>] - c[<span class="hljs-number">0</span>] * s[<span class="hljs-number">1</span>] * s[<span class="hljs-number">2</span>],<br>        c[<span class="hljs-number">0</span>] * c[<span class="hljs-number">1</span>] * c[<span class="hljs-number">2</span>] + s[<span class="hljs-number">0</span>] * s[<span class="hljs-number">1</span>] * s[<span class="hljs-number">2</span>]<br>    );<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>获取关键帧的全局旋转数据</strong></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs Csharp"><span class="hljs-function"><span class="hljs-keyword">public</span> Dictionary&lt;<span class="hljs-built_in">string</span>,Quaternion&gt; <span class="hljs-title">getKeyFrame</span>(<span class="hljs-params"><span class="hljs-built_in">int</span> frameIdx</span>)</span><br>&#123;<br>    Dictionary&lt;<span class="hljs-built_in">string</span>, <span class="hljs-built_in">string</span>&gt; hierachy = getHierachy();<br>    Dictionary&lt;<span class="hljs-built_in">string</span>, Quaternion&gt; boneData = <span class="hljs-keyword">new</span> Dictionary&lt;<span class="hljs-built_in">string</span>, Quaternion&gt;();<br>    boneData.Add(<span class="hljs-string">&quot;pos&quot;</span>, <span class="hljs-keyword">new</span> Quaternion(<br>        boneList[<span class="hljs-number">0</span>].channels[<span class="hljs-number">0</span>].values[frameIdx],<br>        boneList[<span class="hljs-number">0</span>].channels[<span class="hljs-number">1</span>].values[frameIdx],<br>        boneList[<span class="hljs-number">0</span>].channels[<span class="hljs-number">2</span>].values[frameIdx],<span class="hljs-number">0</span>));<br><br>    boneData.Add(boneList[<span class="hljs-number">0</span>].name, eul2quat(<br>        boneList[<span class="hljs-number">0</span>].channels[<span class="hljs-number">3</span>].values[frameIdx],<br>        boneList[<span class="hljs-number">0</span>].channels[<span class="hljs-number">4</span>].values[frameIdx],<br>        boneList[<span class="hljs-number">0</span>].channels[<span class="hljs-number">5</span>].values[frameIdx]));<br>    <span class="hljs-keyword">foreach</span> (BVHBone bb <span class="hljs-keyword">in</span> boneList)<br>    &#123;<br>        <span class="hljs-keyword">if</span> (bb.name != boneList[<span class="hljs-number">0</span>].name)<br>        &#123;<br>            Quaternion localrot = eul2quat(bb.channels[<span class="hljs-number">3</span>].values[frameIdx],<br>                                           bb.channels[<span class="hljs-number">4</span>].values[frameIdx],<br>                                           bb.channels[<span class="hljs-number">5</span>].values[frameIdx]);<br>            boneData.Add(bb.name, boneData[hierachy[bb.name]] * localrot);<br>        &#125;                <br>    &#125;            <br>    <span class="hljs-keyword">return</span> boneData;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>获取 BVH 初始姿势每个关节相对于父关节的偏移量</strong></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Csharp"><span class="hljs-function"><span class="hljs-keyword">public</span> Dictionary&lt;<span class="hljs-built_in">string</span>,Vector3&gt; <span class="hljs-title">getOffset</span>(<span class="hljs-params"><span class="hljs-built_in">float</span> ratio</span>)</span> &#123;<br>    Dictionary&lt;<span class="hljs-built_in">string</span>, Vector3&gt; offset = <span class="hljs-keyword">new</span> Dictionary&lt;<span class="hljs-built_in">string</span>, Vector3&gt;();<br>    <span class="hljs-keyword">foreach</span>(BVHBone bb <span class="hljs-keyword">in</span> boneList)<br>    &#123;<br>        offset.Add(bb.name, <span class="hljs-keyword">new</span> Vector3(bb.offsetX * ratio, bb.offsetY * ratio, bb.offsetZ * ratio));<br>    &#125;<br>    <span class="hljs-keyword">return</span> offset;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>获取 BVH 的全局旋转，即 $T_{2}$</strong></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Csharp">bvhT = bp.getKeyFrame(<span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><p><strong>计算 $T_{5}$</strong></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Csharp"><span class="hljs-keyword">foreach</span> (BoneMap bm <span class="hljs-keyword">in</span> bonemaps)<br>&#123;<br>    Transform currBone = anim.GetBoneTransform(bm.humanoid_bone);<br>    currBone.rotation = (currFrame[bm.bvh_name] * Quaternion.Inverse(bvhT[bm.bvh_name])) * unityT[bm.humanoid_bone];<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>确保 bvh 动捕数据第一帧为 Tpose</li><li>运行时 Scene 界面用红线画出了 bvh 动作骨架，可用以鉴别 bvh 动作是否导入成功</li><li>若使用的 bvh 文件的旋转量不是 ZYX 顺序，请相应修改 BVHParser.cs 中的 eul2quat 函数，一般只需调换该函数参数顺序即可</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文先简要介绍了几个基础知识 BVH 和角色姿势，后阐述了 BVH 驱动动作生成的理论（以 Tpose 为中间媒介求解转换矩阵），并使用代码实现。</p><p>项目地址见 <a href="https://github.com/ZewanHuang/unitypackages/tree/master/3_BVHParser">BVHParser</a>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/zb1165048017/article/details/112394097">[1] Unity 中 BVH 骨骼动画驱动的可视化理论与实现 - CSDN</a></p><p><a href="https://baike.baidu.com/item/bvh/3691673">[2] BVH - 百度百科</a></p>]]></content>
    
    
    <categories>
      
      <category>Unity</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Unity</tag>
      
      <tag>bvh</tag>
      
      <tag>3D动画</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>正规方程的推导</title>
    <link href="/2021/07/13/ml/normalEqu/"/>
    <url>/2021/07/13/ml/normalEqu/</url>
    
    <content type="html"><![CDATA[<p>本篇主要介绍正规方程公式的推导，关于正规方程详情，可参考 <a href="https://blog.zewan.cc/2021/07/12/ml-regression/#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B-Normal-Equation">线性回归 - Zewan Blog</a>。</p><h2 id="线性代数基础"><a href="#线性代数基础" class="headerlink" title="线性代数基础"></a>线性代数基础</h2><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>设 $A$ 为 $m\times p$ 的矩阵，$B$ 为 $p\times n$ 的矩阵，则称 $m\times n$ 的矩阵 $C$ 为 $A$ 与 $B$ 的乘积，记作 $C=AB$，其中矩阵 $C$ 的第 $i$ 行第 $j$ 列元素为：</p><p>$$<br>(AB)_{ij} =<br>\sum^{p}_{k=1}<br>a_{ik}b_{kj}=a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{ip}b_{pj}<br>$$</p><p>例如：</p><p>$$<br>A =<br>\left[<br>\begin{matrix}<br>1 &amp; 2 &amp; 3  \\<br>4 &amp; 5 &amp; 6  \\<br>\end{matrix}<br>\right];<br>B =<br>\left[<br>\begin{matrix}<br>7 &amp; 8   \\<br>9 &amp; 10  \\<br>11 &amp; 12<br>\end{matrix}<br>\right]<br>$$</p><p>则 $A$ 与 $B$ 的乘积为：</p><p>$$<br>C=AB=<br>\left[<br>\begin{matrix}<br>1\times 7+2\times 9+3\times 11 &amp; 1\times 8+2\times 10+3\times 12   \\<br>4\times 7+5\times 9+6\times 11 &amp; 4\times 8+5\times 10+6\times 12<br>\end{matrix}<br>\right]<br>$$</p><h3 id="矩阵的可逆性"><a href="#矩阵的可逆性" class="headerlink" title="矩阵的可逆性"></a>矩阵的可逆性</h3><p>给定一个 $n$ 阶<strong>方阵</strong> $A$，若存在一个 $n$ 阶方阵 $B$ 使得 $AB=BA=I$，其中 $I$ 为单位矩阵，则称 $A$ 可逆，且 $B$ 为 $A$ 的逆矩阵，记作 $A^{-1}$。</p><p>若方阵 $A$ 的逆矩阵存在，则称 $A$ 可逆，为非奇异矩阵，其列向量必线性无关。</p><blockquote><p>奇异矩阵即为不可逆矩阵</p></blockquote><div class="note note-warning">            <p>注意可逆性是对于方阵而言，非方阵矩阵（行列数不同）必不可逆</p>          </div><h3 id="矩阵求导法则"><a href="#矩阵求导法则" class="headerlink" title="矩阵求导法则"></a>矩阵求导法则</h3><p>具体知识可见 <a href="https://blog.zewan.cc/2021/07/18/matrix-derivation">矩阵求导 - Zewan Blog</a>，此处仅介绍需要用到的两个<strong>标量对向量求导</strong>的恒等式推导。</p><p><strong>等式一：</strong>$\mathbf{a},\mathbf{x}$ 为 $n$ 维列向量，则 $\frac{\partial \mathbf{a^{T}x}}{\partial \mathbf{x}}=\mathbf{a}$</p><p>根据定义，对 $\mathbf{x}$ 的第 $i$ 个分量求导：</p><p>$$<br>\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial x_{i}}<br>=<br>\frac{\partial \sum_{j=1}^{n} a_{j} x_{j}}{\partial x_{i}}<br>=<br>\frac{\partial a_{i} x_{i}}{\partial x_{i}}<br>=<br>a_{i}<br>$$</p><p>可见，对向量的第 $i$ 个分量的求导结果等于向量 $\mathbf{a}$ 的第 $i$ 个分量。采用分母布局，求导结果组成 $\mathbf{a}$。即：</p><p>$$<br>\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial \mathbf{x}}<br>=<br>\mathbf{a}\tag{1}<br>$$</p><p><strong>等式二：</strong>$\mathbf{x}$ 为 $n$ 维列向量，$\mathbf{A}$ 为 $n\times n$ 方阵，则 $\frac{\partial \mathbf{x}^{T} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}=\mathbf{A}^{T}\mathbf{x}+\mathbf{Ax}$</p><p>对 $\mathbf{x}$ 的第 $k$ 个分量进行求导如下：</p><p>$$<br>\frac{\partial \mathbf{x^{T}Ax}}{\partial x_{k}}<br>=<br>\frac{\partial \sum_{i=1}^{n} \sum_{j=1}^{n} x_{i}A_{ij}x_{j}}{\partial x_{k}}<br>=<br>\sum_{i=1}^{n}A_{ik}x_{i}<br>+<br>\sum_{j=1}^{n}A_{kj}x_{j}<br>$$</p><p>将这 $k$ 个分量组成向量，即得到：</p><p>$$<br>\frac{\partial \mathbf{x}^{T} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}<br>=<br>\mathbf{A}^{T}\mathbf{x}<br>+<br>\mathbf{Ax}<br>\tag{2}<br>$$</p><h2 id="正规方程简述"><a href="#正规方程简述" class="headerlink" title="正规方程简述"></a>正规方程简述</h2><p>训练集中有 $m$ 个样本，每个样本有 $n$ 个线性无关的特征和一个标签纸（输出值）$y$。建立多变量线性回归模型和代价函数：($x_{0}$ 为 1)</p><ul><li><p>回归模型：$h_{\theta}(x)=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}=\theta^{T}X$</p></li><li><p>代价函数：$J(\theta_{0},\theta_{1},…,\theta_{n})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$</p></li></ul><p>现使用特征 $\frac{\partial J(\theta)}{\partial\theta}=0$，推导出参数方程：</p><p>$$<br>\theta=(X^{T}X)^{-1}X^{T}y<br>$$</p><p>其中，$X$ 表示特征矩阵（含$x_{0}=1$），$T$ 表示矩阵转置，$y$ 为训练集的结果，是一个向量。即：</p><p>$$<br>\theta =<br>\left[<br>\begin{matrix}<br>\theta_{0} \\<br>\theta_{1} \\<br>. \\<br>. \\<br>\theta_{n}<br>\end{matrix}<br>\right];<br>X =<br>\left[<br>\begin{matrix}<br>x_{0}^{(1)} &amp; \cdots &amp; x_{n}^{(1)} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>x_{0}^{m} &amp; \cdots &amp; x_{n}^{m}<br>\end{matrix}<br>\right];<br>y =<br>\left[<br>\begin{matrix}<br>y_{1} \\<br>y_{2} \\<br>. \\<br>. \\<br>y_{m}<br>\end{matrix}<br>\right]<br>$$</p><h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>将代价函数 $J(\theta)$ 转为矩阵表达形式，有</p><p>$$<br>J(\theta) = \frac{1}{2}(\mathbf{X \theta}-\mathbf{y})^{2}<br>$$</p><blockquote><p>上式 $\mathbf{X \theta}-\mathbf{y}$ 为 $m$ 维列向量，平方仅代表各元素做平方运算</p></blockquote><p>对 $J(\theta)$ 进行如下变换：</p><p>$$<br>\begin{aligned}<br>J(\theta) &amp;=  \frac{1}{2}(\mathbf{X\theta-y})^{T}(\mathbf{X\theta-y})\\<br>&amp;= \frac{1}{2}(\theta^{T}\mathbf{X}^{T}-\mathbf{y}^{T})(\mathbf{X}\theta-\mathbf{y})\\<br>&amp;= \frac{1}{2}(\theta^{T}\mathbf{X}^{T}\mathbf{X}\theta-\theta^{T}\mathbf{X}^{T}\mathbf{y}-\mathbf{y}^{T}\mathbf{X}\theta+\mathbf{y}^{T}\mathbf{y})<br>\end{aligned}<br>$$</p><p>接下来使用矩阵的求导法则 $\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial \mathbf{x}}=\mathbf{a},\frac{\partial \mathbf{x}^{T} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}=\mathbf{A}^{T}\mathbf{x}+\mathbf{Ax}$，对 $J(\theta)$ 偏导:</p><p>$$<br>\frac{\partial \theta^{T} \mathbf{X}^{T} \mathbf{X} \theta}{\partial \theta}<br>=<br>(\mathbf{X}^{T}\mathbf{X})^{T}\theta<br>+<br>(\mathbf{X}^{T}\mathbf{X})\theta<br>=<br>2\mathbf{X}^{T}\mathbf{X}<br>$$</p><p>得：</p><p>$$<br>\begin{aligned}<br>\frac{\partial J(\theta)}{\partial \theta}<br>&amp;= \frac{1}{2}(2\mathbf{\mathbf{X}}^{T}\mathbf{X}\theta-\mathbf{X}^{T}\mathbf{\mathbf{y}}-(\mathbf{y}^{T}\mathbf{X})^{T}+0)\\<br>&amp;= \frac{1}{2}(2\mathbf{X}^{T}\mathbf{X}\theta-\mathbf{X}^{T}\mathbf{y}-\mathbf{X}^{T}\mathbf{y}+0)\\<br>&amp;= \mathbf{X}^{T}\mathbf{X}\theta-\mathbf{X}^{T}\mathbf{y}<br>\end{aligned}<br>$$</p><p>令 $\frac{\partial J(\theta)}{\partial \theta}=0$，得</p><p>$$<br>\theta = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}<br>$$</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.coursera.org/learn/machine-learning">[1] 吴恩达 Andrew Ng 机器学习课程</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">[2] 黄海广博士的机器学习笔记</a></p><p><a href="https://zhuanlan.zhihu.com/p/60719445">[3] 详解正规方程 (Normal Equation) - 知乎</a></p><p><a href="https://fei-wang.github.io/matrix.html">[4] 矩阵求导 - 蒙奇 D 路飞</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习『吴恩达』</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>ML</tag>
      
      <tag>推导</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性回归</title>
    <link href="/2021/07/12/ml/regression/"/>
    <url>/2021/07/12/ml/regression/</url>
    
    <content type="html"><![CDATA[<h2 id="单变量线性回归-Linear-Regression-with-One-Variable"><a href="#单变量线性回归-Linear-Regression-with-One-Variable" class="headerlink" title="单变量线性回归 (Linear Regression with One Variable)"></a>单变量线性回归 (Linear Regression with One Variable)</h2><p>我们使用上一章房价的例子，介绍模型表示、代价函数、梯度下降的一些基本概念。</p><h3 id="模型表示-Model-Representation"><a href="#模型表示-Model-Representation" class="headerlink" title="模型表示 (Model Representation)"></a>模型表示 (Model Representation)</h3><p>首先，对于房价这个监督学习的例子，我们有一个数据集，称为<strong>训练集</strong>。假设数据如下：</p><table><thead><tr><th align="center">Size(x)</th><th align="center">Price in 1w’s(y)</th></tr></thead><tbody><tr><td align="center">101</td><td align="center">98</td></tr><tr><td align="center">134</td><td align="center">156</td></tr><tr><td align="center">201</td><td align="center">237</td></tr><tr><td align="center">…</td><td align="center">…</td></tr></tbody></table><p>用以下符号来描述这个回归问题：</p><ul><li>$m$ 代表训练集中实例的数量</li><li>$x$ 代表特征/输入变量</li><li>$y$ 代表目标变量/输出变量</li><li>$(x,y)$ 代表训练集中的实例</li><li>$(x^{(i)},y^{(i)})$ 代表第$i$个观察实例</li><li>$h$ 代表学习算法的解决方案或函数，也称为<strong>假设 (hypothesis)</strong></li></ul><p><img src="/img/articles/21-7-11/ml-flow.jpg" alt="监督学习算法的工作流程"></p><p>上图展示了监督学习算法的工作方式。可以看到，训练集里有房屋尺寸及其对应的价格，我们将它 feed 给学习算法，然后输出一个函数 $h$。这时我们输入房屋大小 $x$，$h$ 则会输出对应的房屋价格预测值 $y$。因此 $h$ 是一个从 $x$ 到 $y$ 的函数映射。</p><p>如何表达 $h$ 呢？我们可以用简单的线性回归，将其表示为 $h_{\theta}(x)=\theta_{0}+\theta_{1}x$，因为只含有一个特征变量，因此叫做<strong>单变量线性回归问题</strong>。</p><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 (Cost Function)"></a>代价函数 (Cost Function)</h3><p>在上一部分我们为模型引入了<strong>参数</strong> $\theta_{0}$ 和 $\theta_{1}$，在房价问题的例子中表示直线在 $y$ 轴上的截距和直线的斜率。接下来我们要做的便是为模型选择合适的参数 (parameters)。事实上这就是 feed 的过程。</p><p>选择的参数决定了我们得到的直线相对于我们训练集的准确程度，我们称模型的预测值与实际值的差距为建模误差 (modeling error)。此时目标是选择出使得建模误差的平方和最小的模型参数。</p><p>定义<strong>代价函数</strong>表示建模误差：</p><p>$$<br>J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2<br>$$</p><p>我们知道，对特定的 $\theta_{0}$ 和 $\theta_{1}$，我们能得到对应的代价值 $J$。因此 $J$ 是关于 $\theta_{0}$ 和 $\theta_{1}$ 的函数。将其绘制成一个等高线图，三个坐标分别为 $\theta_{0}$、$\theta_{1}$ 和 $J(\theta_{0},\theta_{1})$：</p><p><img src="/img/articles/21-7-11/lineheight.png" alt="图源于Coursera"></p><p>可以直观地看到，三维空间中存在一个使 $J(\theta_{0},\theta_{1})$ 最小的点。</p><p>在这里罗列一下前面的概念：</p><table><thead><tr><th align="center">Concept</th><th align="center">Meaning</th></tr></thead><tbody><tr><td align="center">Hypothesis</td><td align="center">$h_{\theta}(x) = \theta_{0} + \theta_{1} x$</td></tr><tr><td align="center">Parameters</td><td align="center">$\theta_{0},\theta_{1}$</td></tr><tr><td align="center">Cost Function</td><td align="center">$J(\theta_{0},\theta_{1})=\frac{1}{2m}\Sigma_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$</td></tr><tr><td align="center">Goal</td><td align="center">minimize $J(\theta_{0},\theta_{1})$</td></tr></tbody></table><h3 id="参数求解-Parameter-Learning"><a href="#参数求解-Parameter-Learning" class="headerlink" title="参数求解 (Parameter Learning)"></a>参数求解 (Parameter Learning)</h3><p><strong>梯度下降</strong> (Gradient Descent) 是一个求函数最小值的算法，下面将使用该算法求出代价函数 $J(\theta_{0},\theta_{1})$ 的最小值。</p><p>梯度下降：开始时随机选择一个参数组合 $(\theta_{0}, \theta_{1},…,\theta_{n})$，计算代价函数值，接下来寻找下一个能使代价函数值下降最多的参数组合，持续这么做直到到达一个局部最优解 (local optimal solution)。因此我们没有遍历所有参数组合（也不可能遍历），因此不确定得到的解是否为全局最优。</p><p><img src="/img/articles/21-7-11/Gradient.jpg" alt="图源于Coursera"></p><p>梯度下降的思想比较好理解。想象你站立在山的某个点，然后旋转 360 度，看看往哪个方向能下山，然后迈出一定距离的步伐。迈出一步后，重新环顾四周，持续上面的过程，直到到达某个点，发现四周都是上山的方向。</p><p><strong>批量梯度下降</strong> (batch gradient descent) 算法公式为：</p><p>$$<br>\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1}), j=0或1<br>$$</p><p>其中 $\alpha$ 是<strong>学习率</strong> (learning rate)，决定每一次迈出的步子有多大。当 $\alpha$ 太小或太大时：</p><ul><li>$\alpha$ 太小，即学习速率低，达到局部最优的速度会很慢；</li><li>$\alpha$ 太大，即学习速率过快，迈的步子太大，可能会导致多次迭代都越过了局部最优点。</li></ul><p>在批量梯度下降中，我们每次都<strong>同时</strong>让所有参数减去学习速率乘代价函数的偏导。这里的同时，表示先根据当前点计算出式子右边的值（参数的新值），再同时更新参数。</p><p>在计算过程中，我们会发现，每次迭代（计算代价函数值）都会用到所有训练样本，因此该算法称为<strong>批量梯度下降</strong>。当然也存在其它类型的梯度下降法，不考虑计算整个训练集，而是每次只关注训练集的小子集。</p><h2 id="多变量线性回归-Linear-Regression-with-Multiple-Variables"><a href="#多变量线性回归-Linear-Regression-with-Multiple-Variables" class="headerlink" title="多变量线性回归 (Linear Regression with Multiple Variables)"></a>多变量线性回归 (Linear Regression with Multiple Variables)</h2><p>继续延续上面房价模型的例子，不同的是，房价往往与多个变量有关，比如大小、卧室数量、楼层数、房屋年龄等有关，因此我们引入多变量回归问题。</p><h3 id="多维特征-Multiple-Features"><a href="#多维特征-Multiple-Features" class="headerlink" title="多维特征 (Multiple Features)"></a>多维特征 (Multiple Features)</h3><p>建立模型，我们首先需要将其变量进行抽象表示。假设我们有这样的数据：</p><table><thead><tr><th align="center">Size (meter2)</th><th align="center">Number of bedrooms</th><th align="center">Number of floors</th><th align="center">Age of home (years)</th><th align="center">Price (1w)</th></tr></thead><tbody><tr><td align="center">2104</td><td align="center">5</td><td align="center">1</td><td align="center">45</td><td align="center">460</td></tr><tr><td align="center">1416</td><td align="center">3</td><td align="center">2</td><td align="center">40</td><td align="center">232</td></tr><tr><td align="center">1534</td><td align="center">3</td><td align="center">2</td><td align="center">30</td><td align="center">315</td></tr><tr><td align="center">852</td><td align="center">2</td><td align="center">1</td><td align="center">36</td><td align="center">178</td></tr><tr><td align="center">…</td><td align="center">…</td><td align="center">…</td><td align="center">…</td><td align="center">…</td></tr></tbody></table><p>同样用符号表示：</p><ul><li>$m$ 代表训练实例的数量</li><li>$n$ 代表特征的数量</li><li>$x^{(i)}$ 代表第 $i$ 个训练实例</li><li>$x^{(i)}_{j}$ 表示第 $i$ 个训练实例的第 $j$ 个特征</li><li>$X$ 代表特征矩阵，由 $x_{j}^{(i)}$ 组成，第 $i$ 行第 $j$ 列的元素为 $x^{(i)}_{j}$ </li></ul><p>我们可以将假设定义为多变量线性函数：</p><p>$$<br>h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}<br>$$</p><p>为简化公式，引入 $x_{0}=1$，则公式转化为：</p><p>$$<br>h_{\theta}(x)=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}<br>$$</p><p>即</p><p>$$<br>h_{\theta}(x)=<br>\left[<br>\begin{matrix}<br>\theta_{0} &amp; \theta_{1} &amp; … &amp; \theta_{n}<br>\end{matrix}<br>\right]<br>\left[<br>\begin{matrix}<br>x_{0} \\<br>x_{1} \\<br>. \\<br>. \\<br>x_{n}<br>\end{matrix}<br>\right]<br>=<br>\theta^{T}x<br>$$</p><blockquote><p>上标 $T$ 代表矩阵转置</p></blockquote><h3 id="多变量梯度下降"><a href="#多变量梯度下降" class="headerlink" title="多变量梯度下降"></a>多变量梯度下降</h3><h4 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h4><p>同样的，我们为该问题构建一个代价函数，即所有建模误差的平方和：</p><p>$$<br>J(\theta_{0},\theta_{1},…,\theta_{n})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2<br>$$</p><p>其中，$h_{\theta}(x)=\theta^{T}x=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$</p><p>此处给出简单的 <strong>Python</strong> 代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost</span>(<span class="hljs-params">X, y, theta</span>):</span><br>    inner = np.power(((X * theta.T) - y), <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(inner) / (<span class="hljs-number">2</span> * <span class="hljs-built_in">len</span>(X))<br></code></pre></td></tr></table></figure><p>目标同样是找出使得代价函数最小的一系列参数。多变量线性回归的<strong>批量梯度下降算法</strong>为：</p><p>$$<br>Repeat:<br>\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1},…,\theta_{n})<br>$$</p><p>代入函数求导，得到：</p><p>$$<br>\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_{\theta}(x^{(i)})-y^{(i)})\cdot x_{j}^{i})<br>$$</p><p>一开始随机选择一系列参数值，计算预测结果，再由上式计算得到所有参数的新值，同时更新，如此循环直至收敛。</p><h4 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h4><p>继续考虑一下数据的问题。假设房价问题有两种特征数据，房屋尺寸和房间数量，尺寸的值为 0-2000 平方米，而房间数量值为 0-10。</p><p>可以发现，这两个参数的范围相差巨大，绘制代价函数时，图像会显得很扁，梯度下降算法需要非常多次迭代才能收敛。</p><p>解决的方法是将所有特征的尺度都尽量缩放到 -1 到 1 之间，保证特征在相似的范围内。比如：</p><p>$$<br>x_1 = \frac{size}{2000},x_2=\frac{nums}{5}<br>$$</p><p>当然，更常用和简单的方法是令：$x_{i}=\frac{x_{i}-\mu_{i}}{s_{i}}$，其中 $\mu_{i}$ 是第 $i$ 个特征的平均值，$s_{i}$ 是第 $i$ 个特征的方差。</p><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们可以绘制图标来观测算法在何时趋于收敛。如下图，纵轴为代价函数值，横轴为迭代次数：</p><p><img src="/img/articles/21-7-11/iterations.jpg"></p><p>使用梯度下降算法，该图像应整体呈下降趋势，最后趋于收敛。</p><p>关于学习率 $\alpha$ 的选择，如果 $\alpha$ 过小，则迭代次数非常高，如果 $\alpha$ 过大，每次迭代可能不会减小代价函数值，可能会越过局部最小值导致无法收敛。</p><p>通常可以考虑尝试这些学习率：</p><p>$$<br>\alpha = 0.01,0.03,0.1,0.3,1,3,10<br>$$</p><h4 id="多项式回归-Polynomial-Regression"><a href="#多项式回归-Polynomial-Regression" class="headerlink" title="多项式回归 (Polynomial Regression)"></a>多项式回归 (Polynomial Regression)</h4><p>上面所使用的线性回归，不一定适用于所有数据，有时我们需要曲线来适应拟合数据。比如二次方模型：$h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^{2}$ 或三次方模型等等。</p><p><img src="/img/articles/21-7-11/polynomial.jpg" alt="图源于Coursera"></p><p>通常我们需要先观察数据，再决定模型。如果采用多次项，可以令：$x_{2}=x_{2}^{2},x_3=x_{3}^{3}$，从而将模型转化为线性回归模型。</p><h3 id="正规方程-Normal-Equation"><a href="#正规方程-Normal-Equation" class="headerlink" title="正规方程 (Normal Equation)"></a>正规方程 (Normal Equation)</h3><p>到目前为止，都在使用梯度下降算法来求解参数，但对于某些线性回归问题，<strong>正规方程</strong>方法是更好的解法。</p><div class="note note-info">            <p>最小二乘法可以将误差方程转化为有确定解的代数方程组（方程式数目等于未知数的个数），从而可求解出这些位置参数。这个有确定解的代数方程组称为最小二乘法估计的<strong>正规方程（或法方程）</strong>。</p>          </div><p>看不懂上面的阐述没关系，我们依然拿房价的例子来理解一下。</p><p>首先，上述定义中的误差方程，即指前面所建立的代价函数。它所说的最小二乘法，正是我们用以建模的思想。这两个条件都满足的前提下，我们需要思考如何将代价函数转化为所谓的有确定解的方程组。</p><p><img src="/img/articles/21-7-11/partial.jpg"></p><p>这里运用了数学中极值点的思想。我们想找到代价函数最小的解，利用 $\frac{\partial}{\partial\theta_{j}}J(\theta_{j})=0$。共有 $n+1$ 个未知数 $\theta_{0},\theta_{1},..,\theta_{n}$，对每个未知数求偏导，得到 $n+1$ 个方程，即得到了有确定解的方程组。</p><p>$$<br>\frac{\partial}{\partial\theta_{j}}J(\theta_{j})=0<br>$$</p><p>利用上式，结合 $J(\theta)$ 的定义，我们得到非常重要且常用的公式：</p><blockquote><p>推导过程见<a href="https://blog.zewan.cc/2021/07/13/normalEqu/">正规方程的推导 - Zewan Blog</a></p></blockquote><p>$$<br>\theta=(X^{T}X)^{-1}X^{T}y<br>$$</p><p>其中，$X$ 表示特征矩阵（含$x_{0}=1$），$T$ 表示矩阵转置，$y$ 为训练集的结果，是一个向量。</p><p>使用上述公式，即可求解参数。Python 代码示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">normalEqn</span>(<span class="hljs-params">X, y</span>):</span><br>    <span class="hljs-keyword">return</span> np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) <br>    <span class="hljs-comment"># np.linalg.inv 求逆，X.T 转置，.dot(X) 矩阵乘法，可 @ 代替</span><br></code></pre></td></tr></table></figure><p>当然，对于不可逆矩阵，通常是因为特征之间不独立，如同时包含以英尺为单位的特征和以米为单位的特征，或特征数量大于训练集数量，或存在<a href="https://baike.baidu.com/item/%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3/6416511">线性相关</a>的特征，此时正规方程方法不可用。</p><p>梯度下降与正规方程的比较：</p><table><thead><tr><th>Item</th><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>是否需要选择学习率 $\alpha$</td><td>需要</td><td>不需要</td></tr><tr><td>计算过程</td><td>多次迭代</td><td>一次运算得出</td></tr><tr><td>特征数量 $n$</td><td>当特征数量 $n$ 大时也较适用</td><td>计算矩阵逆时间复杂度为 $O(n^{3})$，当 $n&lt;10000$ 时可以接受</td></tr><tr><td>适用性</td><td>适用于各种类型的模型</td><td>仅适用于线性模型</td></tr></tbody></table><p>关于两种方法的选择，当特征变量的数目不大，小于一万时，选择正规方程法，其它情况下还是选用梯度下降法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上述是机器学习中线性回归问题的阐述。可以看到，针对房价这类问题，流程如下：</p><ol><li>根据数据特征建立模型（本文建立的是线性模型）</li><li>定义代价函数表示建模误差</li><li>使用求函数最小值的算法求解参数<ol><li>正规方程：当特征变量较少时选用</li><li>梯度下降：当特征变量多，或为非线性模型时采用，需选择合适的学习率</li></ol></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.coursera.org/learn/machine-learning">[1] 吴恩达 Andrew Ng 机器学习课程</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">[2] 黄海广博士的机器学习笔记</a></p><p><a href="https://baike.baidu.com/item/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/10001812">[3] 正规方程 - 百度百科</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习『吴恩达』</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习的基础分类与概念</title>
    <link href="/2021/07/10/ml/welcome/"/>
    <url>/2021/07/10/ml/welcome/</url>
    
    <content type="html"><![CDATA[<blockquote><p>使用深度学习框架完成了一个项目，感觉对机器学习的基础掌握得零零碎碎，趁暑假冲一波。</p></blockquote><p>本篇文章是机器学习系列的第一篇博客，讲述机器学习是什么、监督学习与无监督学习，并以单变量线性回归为例，介绍模型、代价函数与参数学习（梯度下降）。</p><h2 id="机器学习是什么"><a href="#机器学习是什么" class="headerlink" title="机器学习是什么"></a>机器学习是什么</h2><p>我们每天都在不知不觉中使用了机器学习的应用，比如使用百度能搜索到你需要的内容，邮箱能自动将部分邮件扔进垃圾邮件中，手机程序能认出你的照片并将其归类，等等。</p><p>机器学习作为一门多领域交叉学科，涉及各个行业和基础科学，受到广泛应用。</p><p><img src="/img/articles/21-7-11/ml-apply.png"></p><p>与普通程序不同的是，它可以理解为<strong>让机器自己学习怎样解决问题</strong>。例如在图像识别问题上，简单地让计算机执行固有功能流程，并不能很好地识别图片，这时唯一方法是让计算机学习怎样识别。</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>机器学习有两个定义：</p><div class="note note-info">            <p><strong>Maching Learning Definition</strong></p><ol><li>Arthor Samuel(1959): Field of study that gives computers the ability to learn without being explicitly programmed.</li><li>Tom Mitchell(1998): Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</li></ol>          </div><p>Arthor Samuel 提出，机器学习是使计算机能够在没有明确编程的情况下学习的研究领域。他编写了一个西洋棋程序，让程序自己跟自己下了上万盘棋，久而久之，程序明白了什么是好的布局，于是玩西洋棋的水平得到了很大的提升。</p><p>Tom Mitchell 认为，一个好的机器学习程序是，能从经验 E 中学习，解决任务 T，达到性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。</p><p>以下棋为例，任务 T 就是下棋，经验 E 是程序上万次的自我练习后所获得的经验，而 P 即为棋局布局的好坏度量。程序在多次下棋中积累经验，经过布局好坏度量进行评判，从而提升玩西洋棋的水平。</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>机器学习有以下几类：</p><ul><li>监督学习 (Supervised Learning)：我们教计算机如何去学习</li><li>无监督学习 (Unsupervised Learning)：计算机自己学习</li><li>其它：强化学习 (Reinforcement Learning)、推荐算法 (recommender systems)</li></ul><p>具体将在下面分别介绍。</p><h2 id="监督学习-Supervised-Learning"><a href="#监督学习-Supervised-Learning" class="headerlink" title="监督学习 (Supervised Learning)"></a>监督学习 (Supervised Learning)</h2><p>前面我们说到，监督学习是程序员教计算机如何去学习，该特点体现为，<strong>我们所提供训练的数据是明确正确答案的</strong>。大家应该都有所了解，机器学习常常由 code 和 data 组成，我们使用数据集去训练机器，从而实现功能。监督学习的特征正是 right answers given。</p><p>监督学习常见的问题有回归问题、分类问题，我将在分别举例介绍后归纳监督学习的特征。</p><h3 id="回归问题-Regression"><a href="#回归问题-Regression" class="headerlink" title="回归问题 (Regression)"></a>回归问题 (Regression)</h3><p>对数学有所认识的人应该都了解过线性回归，而事实上它正是回归问题的一种。</p><p>举个简单的例子，现在已收集了一些北京房价的数据，并将其画出坐标图，横轴表示房子的面积，纵轴表示房价。基于这组数据，假如你想在北京买套 250 平方米的房子，大致价格是多少。</p><p><img src="/img/articles/21-7-11/regression.png"></p><p>使用学习算法，我们可以用一条直线拟合这些数据，根据该直线可以推测出房子大概需要 280w。当然这不是唯一的算法，也不是最好的算法。我们还可以用二次曲线拟合，推测价格为 310 w。以上就是监督学习的回归问题的例子。</p><p>可以看到，监督学习指的是，我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价例子中，我们给出了一系列房子的数据，这些数据包含每个样本的正确价格（实际售价），即正确答案。</p><p>而回归一词研究的是<strong>数据的连续性</strong>。房价实际上是离散值，但研究一系列房价时，我们通常将其看成连续的数值，从而推断其间的值。</p><h3 id="分类问题-Classification"><a href="#分类问题-Classification" class="headerlink" title="分类问题 (Classification)"></a>分类问题 (Classification)</h3><p>同样举例来说明一下分类问题。假设想通过肿瘤大小来推断乳腺癌良性与否，现拥有病历上的数据，即部分肿瘤尺寸对应是否恶性的数据点。</p><p>将其绘制出坐标图。该数据集中，横轴表示肿瘤的大小，纵轴值为 1 和 0，分别表示恶性肿瘤和良性肿瘤。</p><p><img src="/img/articles/21-7-11/classification.jpg"></p><p>我们使用 0 或 1 这样的离散值表示数据的准确值，这类问题我们称之为分类问题 (Classification)。分类指的是，我们的<strong>输出值是离散的</strong>。当然，分类并不要求输出值仅两个值，可以是多个离散的值。</p><p>在一般的机器学习问题中，我们大多遇到的不止一种特征。继续使用乳腺癌的例子，恶性与否并不仅仅取决于肿瘤大小，还与患者年龄、肿块密度等特征有关。因此我们常需要使用更多特征，以保证推测结果正确。</p><p>在机器学习中，使用的是<strong>支持向量机</strong>的算法，能存储无限多个特征，让计算机处理它们。以后应该会更新到该内容。</p><h3 id="回味监督学习"><a href="#回味监督学习" class="headerlink" title="回味监督学习"></a>回味监督学习</h3><p>通过上面两个例子，我们可以知道，监督学习的基本思想是，<strong>数据集中的每个样本都有相应的“正确答案”。</strong>我们再根据这些样本作出预测。而回归问题与分类问题的区别，在于输出值（或预测值、样本值）是连续还是离散的。</p><blockquote><p>我更喜欢把上面输出值（或预测值、样本值）认为是<strong>可能值</strong>。</p></blockquote><h2 id="无监督学习-Unsupervised-Learning"><a href="#无监督学习-Unsupervised-Learning" class="headerlink" title="无监督学习 (Unsupervised Learning)"></a>无监督学习 (Unsupervised Learning)</h2><p>上一部分内容我们知道，监督学习的数据集是有正确答案的，比如乳腺癌问题，每条数据都已标明是阴性或阳性，即良性或恶性肿瘤。</p><p>与监督学习相比，<strong>无监督学习的数据没有所谓对应的正确答案</strong>，我们并没有告诉机器，数据点是什么，数据点也没有任何标签。不妨再拿乳腺癌问题说说，对无监督学习而言，只需要输入一系列特征，不需要告知恶性与否。</p><p>针对数据集，无监督学习能判断出两个或多个不同的聚集簇，我们称之为<strong>聚类算法</strong>。</p><p>聚类算法的应用是十分广阔的。比如百度新闻能在非常多的网络新闻内容中挑出同类、同主题的新闻，组成有关联的新闻，尽管它可能不能给出它的类别信息。还比如市场营销，对客户的分析，算法能将顾客划分到不同的细分市场，方便有效地销售。</p><p>当然，聚类只是无监督学习中的一种，<strong>非聚类算法则允许我们在复杂的信息或环境中寻找框架结构</strong>。比如鸡尾酒会问题 (<a href="https://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party</a>)。</p><p><img src="/img/articles/21-7-11/cocktail.jpg" alt="图源于维基百科"></p><p>鸡尾酒会问题是语音识别方向的典型问题。当我们在一个鸡尾酒会谈话时，周边噪音很大，我们还是可以听到朋友说的内容，在在远处突然有人叫自己的名字，我们也会马上注意到。</p><p>我们可以简化这个问题。放两个麦克风在房间中，麦克风在两个地方，离说话人的距离不同。每个麦克风同时记录两人说话的声音，听起来像是两份录音被叠加到一起。无监督学习能区分出两个音频资源，抽取出各人说话的录音。</p><p><img src="/img/articles/21-7-11/microphone.png" alt="图源于Coursera"></p><p>不管是聚类还是非聚类问题，它们的数据集都是无标记的，属于无监督学习。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了机器学习是什么，列举了两个主流定义，对比说明了监督学习与无监督学习的区别，在于数据是否有“正确答案”。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.coursera.org/learn/machine-learning">[1] 吴恩达 Andrew Ng 机器学习课程</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">[2] 黄海广博士的机器学习笔记</a></p>]]></content>
    
    
    <categories>
      
      <category>机器学习『吴恩达』</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>服务器部署 Nginx + Django + Vue</title>
    <link href="/2021/06/24/deploy/deploy-django-vue/"/>
    <url>/2021/06/24/deploy/deploy-django-vue/</url>
    
    <content type="html"><![CDATA[<p>本篇记录我配置和部署服务器的每一步，主要包括服务器配置 Django 虚拟环境、uWSGI 和 Nginx 的使用以及报错的纠正。以 Linux 服务器为例，因此需要具备一定的 Linux 知识基础。</p><h2 id="服务器预设"><a href="#服务器预设" class="headerlink" title="服务器预设"></a>服务器预设</h2><h3 id="租服务器"><a href="#租服务器" class="headerlink" title="租服务器"></a>租服务器</h3><p>各大云平台，如腾讯云、阿里云、华为云等，都有学生优惠。我这里选择的是腾讯云，原因：控制台界面简洁优雅。</p><p>相关配置仅供参考：</p><p><img src="/img/articles/21-6-24/server.png" alt="服务器配置"></p><ul><li>镜像信息：CentOS 7.6 64bit</li><li>实例规格：CPU 1核，内存 2GB</li><li>磁盘：系统盘 40GB</li><li>流量包套餐：带宽 5Mbps，流量包 1000GB/月（免费）</li></ul><h3 id="SSH-远程连接"><a href="#SSH-远程连接" class="headerlink" title="SSH 远程连接"></a>SSH 远程连接</h3><p>配置 SSH 远程连接，方便本地操作服务器，而无需每次都登录云平台。</p><p>在控制台中点击登录，进入服务器终端。第一步需要初始化超级用户 root 的密码，进入 superuser 权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo passwd       <span class="hljs-comment"># 初始化密码</span><br>su                <span class="hljs-comment"># 切换到root超级用户</span><br></code></pre></td></tr></table></figure><p>修改配置文件，允许密码或密钥远程连接。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim /etc/ssh/sshd_config      <span class="hljs-comment"># 编辑ssh设置文件</span><br></code></pre></td></tr></table></figure><p>在打开的文件中，修改：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">RSAAuthentication yes                       <span class="hljs-comment"># 开启rsa验证，需要添加</span><br>PubkeyAuthentication yes                    <span class="hljs-comment"># 开启公钥登录，一般被注释掉了，去掉前面的#就好</span><br>AuthorizedKeysFile .ssh/authorized_keys     <span class="hljs-comment"># 公钥保存位置，原来就有</span><br>PasswordAuthentication yes                  <span class="hljs-comment"># 开启使用密码登录</span><br></code></pre></td></tr></table></figure><p>保存退出，重启 SSH 服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">service sshd restart        <span class="hljs-comment"># 重启ssh服务</span><br></code></pre></td></tr></table></figure><p>设置完毕后，即可在本地 powershell 或 git bash 连接服务器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh root@&lt;IP address&gt;       <span class="hljs-comment"># IP address 为你服务器的公网IP地址</span><br></code></pre></td></tr></table></figure><p>另外，VScode 的 <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh">Remote - SSH</a> 远程连接插件真香。</p><h3 id="配置公钥"><a href="#配置公钥" class="headerlink" title="配置公钥"></a>配置公钥</h3><p>配置公钥后，本地连接服务器，无需每次都输入密码。</p><p>首先，生成本地电脑的公钥。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t rsa           <span class="hljs-comment"># 打开cmd或powershell输入</span><br></code></pre></td></tr></table></figure><p>默认回车即可，成功后在 <code>C:\Users\用户名\.ssh</code> 文件夹下会生成 <code>id_rsa</code> 和 <code>id_rsa.pub</code>，后者就是本地用户的密钥。打开该文件，复制内容。然后使用 ssh 命令登录远程服务器，在 root 用户根目录下创建 .ssh 文件夹并进入，再创建 authorized_keys 文件，将密钥粘贴进去，之后重启 ssh 服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">service sshd restart        <span class="hljs-comment"># 重启ssh</span><br></code></pre></td></tr></table></figure><h3 id="更新系统软件包"><a href="#更新系统软件包" class="headerlink" title="更新系统软件包"></a>更新系统软件包</h3><p>服务器的预配置都比较古老，依次输入以下命令升级软件包或依赖。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum update -y                               <span class="hljs-comment"># 更新系统软件包</span><br>yum -y groupinstall <span class="hljs-string">&quot;Development tools&quot;</span>     <span class="hljs-comment"># 安装软件管理包</span><br>yum install openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel psmisc libffi-devel epel-release     <span class="hljs-comment"># 安装可能使用的依赖</span><br></code></pre></td></tr></table></figure><h2 id="配置-Django"><a href="#配置-Django" class="headerlink" title="配置 Django"></a>配置 Django</h2><h3 id="安装-python3-8-4"><a href="#安装-python3-8-4" class="headerlink" title="安装 python3.8.4"></a>安装 python3.8.4</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>                   <span class="hljs-comment"># 我一般喜欢把文件下载到该目录下</span><br>wget https://www.python.org/ftp/python/3.8.4/Python-3.8.4.tgz<br>tar -zxvf Python-3.8.4.tgz      <span class="hljs-comment"># 解压python包</span><br></code></pre></td></tr></table></figure><p>进入 Python 包路径，并编译安装到指定路径 /usr/local/python3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> Python-3.8.4<br>./configure --prefix=/usr/<span class="hljs-built_in">local</span>/python3<br>make &amp;&amp; make install<br></code></pre></td></tr></table></figure><p>安装成功后，建立软链接，添加环境变量。因为服务器系统自带有 python、python2、python3，因此我命名为 python3.8，避免冲突。但我的服务器只有 pip3 没有 pip，所以我将 pip3.8 的软连接命名为 pip。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ln -s /usr/<span class="hljs-built_in">local</span>/python3/bin/python3.8 /usr/bin/python3.8<br>ln -s /usr/<span class="hljs-built_in">local</span>/python3/bin/pip3.8 /usr/bin/pip<br></code></pre></td></tr></table></figure><p>检测是否安装成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">python3.8 -V<br>pip -V<br></code></pre></td></tr></table></figure><h3 id="安装虚拟环境"><a href="#安装虚拟环境" class="headerlink" title="安装虚拟环境"></a>安装虚拟环境</h3><p>建议安装虚拟环境 virtualenv，当不同项目要求的 python 版本不同时，不会产生冲突。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install virtualenv<br>pip install virtualenvwrapper       <span class="hljs-comment"># 管理虚拟环境</span><br></code></pre></td></tr></table></figure><p>下载成功后，创建存储虚拟环境的目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkdir ~/.virtualenvs                <span class="hljs-comment"># 我一般存放在 /root/.virtualenvs，可自行修改</span><br></code></pre></td></tr></table></figure><p>查找 <code>virtualenvwrapper.sh</code> 文件位置，添加环境。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">find / -name virtualenvwrapper.sh<br></code></pre></td></tr></table></figure><p>编辑 <code>.bash_profile</code> 文件，在末尾添加这两句，其中 <code>source</code> 后的路径为前面查到的路径。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> WORKON_HOME=<span class="hljs-variable">$HOME</span>/.virtualenvs<br><span class="hljs-built_in">source</span>  /usr/<span class="hljs-built_in">local</span>/python3/bin/virtualenvwrapper.sh<br></code></pre></td></tr></table></figure><p>保存修改后，更新配置信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bash_profile <br></code></pre></td></tr></table></figure><p>如果保存时报错，在 /etc/profile 中加入下面内容，再 <code>source</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3.8<br><span class="hljs-built_in">export</span> VIRTUALENVWRAPPER_VIRTUALENV=/usr/<span class="hljs-built_in">local</span>/python3/bin/virtualenv<br></code></pre></td></tr></table></figure><h3 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h3><p>通过 -p 指定使用的Python版本，创建成功后自动进入该虚拟环境。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mkvirtualenv -p python3.8 django        <span class="hljs-comment"># django为虚拟环境名称</span><br></code></pre></td></tr></table></figure><p>如果你希望将当前虚拟环境安装的所有插件配置到新虚拟环境中，可以执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip freeze &gt; requirements.txt           <span class="hljs-comment"># 导出依赖</span><br>pip install -r requirements.txt         <span class="hljs-comment"># 进入新虚拟环境后再执行</span><br></code></pre></td></tr></table></figure><p><strong>虚拟环境的其它常用命令</strong></p><ul><li>查看创建的全部虚拟环境：<code>workon</code></li><li>使用某一虚拟环境：<code>workon 虚拟环境名称</code></li><li>退出当前虚拟环境：<code>deactivate</code></li><li>删除虚拟环境：<code>rmvirtualenv 虚拟环境名称</code> 记得退出再删除</li></ul><h3 id="虚拟环境中安装-Django-和-uWSGI"><a href="#虚拟环境中安装-Django-和-uWSGI" class="headerlink" title="虚拟环境中安装 Django 和 uWSGI"></a>虚拟环境中安装 Django 和 uWSGI</h3><p>uWSGI 可以理解为服务器上持续运行 Django 的代理服务器，用于与 Django 后端进行数据传输等，后续配置需要使用。</p><p>进入前面创建的虚拟环境，安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install django==3.2         <span class="hljs-comment"># 可指定版本</span><br>pip install uwsgi<br></code></pre></td></tr></table></figure><blockquote><p>uWSGI 要安装两次，一次在虚拟环境中，另一次退出虚拟环境进行安装</p></blockquote><p>创建 uWSGI 的软链接。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ln -s /usr/<span class="hljs-built_in">local</span>/python3/bin/uwsgi /usr/bin/uwsgi<br></code></pre></td></tr></table></figure><h2 id="安装-Nginx"><a href="#安装-Nginx" class="headerlink" title="安装 Nginx"></a>安装 Nginx</h2><p>Nginx 是 Http 反向代理 web 服务器，同时也提供 IMAP/POP3/SMTP 服务，占用内存少，并发能力强。在这里我们只需要了解，Nginx 能帮我们在指定端口跑我们的项目就好了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yum install nginx<br></code></pre></td></tr></table></figure><p>安装成功后，相关的文件存储路径为</p><ul><li>安装成功后，默认的网站目录为 <code>/usr/share/nginx/html</code></li><li>默认的配置文件为 <code>/etc/nginx/nginx.conf</code></li><li>自定义配置文件目录为 <code>/etc/nginx/conf.d/</code></li></ul><p>在启动之前，还需确保服务器的相关端口已打开。http 对应 80 端口，https 对应 443 端口。一般在云平台租的服务器，可以在控制台中的防火墙处开启相应端口。我的设置可供参考。</p><p><img src="/img/articles/21-6-24/duankou.png" alt="服务器端口"></p><p>接下来启动 Nginx</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl start nginx<br></code></pre></td></tr></table></figure><p>启动成功后，浏览器搜索服务器 IP 地址，就能访问到 Nginx 主页了。</p><p><img src="/img/articles/21-6-24/Nginx.jpg" alt="Nginx 默认主页"></p><h2 id="部署项目"><a href="#部署项目" class="headerlink" title="部署项目"></a>部署项目</h2><h3 id="上传项目"><a href="#上传项目" class="headerlink" title="上传项目"></a>上传项目</h3><p>Django 后端项目文件，直接上传至服务器即可。Vue 框架写的前端，需要使用 <code>npm run build</code> 命令进行打包，再将生成的 dist 目录上传。</p><p>这里推荐软件 <a href="https://filezilla-project.org/">FileZilla</a>，用于本地与服务器文件传输十分方便。</p><h3 id="配置-uWSGI"><a href="#配置-uWSGI" class="headerlink" title="配置 uWSGI"></a>配置 uWSGI</h3><p>新建文件 uwsgi.ini，我习惯放置于 Django 项目的根目录下，用于指定项目路径、最大进程数、运行端口等。我的配置参数可供参考。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[uwsgi]</span><br><span class="hljs-attr">socket</span> = <span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">8080</span><br><span class="hljs-attr">chdir</span> = /root/Ops/django<br><span class="hljs-attr">wsgi-file</span> = /root/Ops/django/django3/wsgi.py<br><span class="hljs-attr">master</span> = <span class="hljs-literal">true</span> <br><span class="hljs-attr">enable-threads</span> = <span class="hljs-literal">true</span><br><span class="hljs-attr">processes</span> = <span class="hljs-number">8</span><br><span class="hljs-attr">buffer-size</span> = <span class="hljs-number">65536</span><br><span class="hljs-attr">vacuum</span> = <span class="hljs-literal">true</span><br><span class="hljs-attr">daemonize</span> = /root/Ops/django/uwsgi.log<br><span class="hljs-attr">virtualenv</span> = /root/.virtualenvs/django<br><span class="hljs-attr">uwsgi_read_timeout</span> = <span class="hljs-number">600</span><br><span class="hljs-attr">threads</span> = <span class="hljs-number">4</span><br><span class="hljs-attr">chmod-socket</span> = <span class="hljs-number">664</span><br></code></pre></td></tr></table></figure><p>简要介绍该文件的配置信息：</p><ul><li><code>[uwsgi]</code>：必须有这个[uwsgi]，不然会报错</li><li><code>socket</code>：该端口为后端 Django 的运行端口，可自定义，但须与后面 Nginx 的配置一致</li><li><code>chdir</code>：django 项目路径</li><li><code>wsgi-file</code>：django 项目的 wsgi.py 文件路径</li><li><code>master</code>：开启主进程</li><li><code>processes</code>：最大进程数量</li><li><code>vacuum</code>：当服务器退出的时候自动删除 unix socket 文件和 pid 文件</li><li><code>daemonize</code>：输出日志，有报错时可查看</li><li><code>virtualenv</code>：项目虚拟环境路径</li></ul><p>切换当前路径到 uwsgi.ini 文件所在目录，启动 uWSGI。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">uwsgi --ini uwsgi.ini<br></code></pre></td></tr></table></figure><p>使用 <code>ps</code> 命令查看进程，检测是否成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ps -aux | grep uwsgi<br></code></pre></td></tr></table></figure><p><img src="/img/articles/21-6-24/uwsgi.png" alt="uwsgi 进程查看"></p><h3 id="配置-Nginx"><a href="#配置-Nginx" class="headerlink" title="配置 Nginx"></a>配置 Nginx</h3><blockquote><p>此处先给出部署域名的样例，仅服务器 IP 以后再给出。</p></blockquote><p>首先，删除 <code>/etc/nginx/nginx.conf</code> 文件中 <code>server&#123;...&#125;</code> 部分的代码。当然，如果怕出错，也可先将原本的 nginx.conf 文件备份一下。</p><p>接下来，在 <code>/etc/nginx/conf.d</code> 文件夹中修改默认文件 <code>default.conf</code>（若不存在则新建一个），文件内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs conf">server &#123;<br>    listen 80;<br>    listen 443 ssl;<br>    server_name  zewan.top www.zewan.top;<br><br>    location / &#123;<br>        root /root/Ops/vue/dist;<br>        index index.html index.htm;<br>        try_files $uri $uri/ /index.html;<br>    &#125;<br><br>    location /api &#123;        <br>        include /etc/nginx/uwsgi_params;<br>        uwsgi_pass 127.0.0.1:8080;                                                               <br>    &#125;<br><br>    ssl_certificate /etc/nginx/ssl/zewan.top.crt;<br>    ssl_certificate_key /etc/nginx/ssl/zewan.top.key;<br>    ssl_session_timeout  5m;<br>    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;<br>    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4:!DH:!DHE;<br>    ssl_prefer_server_ciphers  on;<br><br>    error_page 497  https://$host$uri?$args;<br>&#125;<br></code></pre></td></tr></table></figure><p>简要说明文件内容的作用：</p><ul><li><code>listen</code> 后接端口，即设定访问的端口，此处同时开放 80 和 443</li><li><code>server_name</code> 为访问域名</li><li><code>location /</code> 后描述前端 dist 项目文件夹的存放地址，<strong>需根据自身情况修改</strong>，注意 dist 即为前端项目的根目录</li><li><code>location /api</code> 后为后端项目运行端口，注意 <code>uwsgi_pass</code> 后须与之前 uWSGI 的配置保持一致</li><li><code>ssl_certificate[_key]</code> 为 SSL 证书存储路径</li></ul><p><strong>重要提醒</strong></p><p>采用 <code>location /api</code> 与 uWSGI 连接，最终将后端运行在 <code>:443/api/</code>。需保证后端的路由都是 <code>api/*</code>，即 Django 项目的 <code>urls.py</code> 文件所有路由前需加 <code>api/</code>。</p><h2 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h2><p>检测 Nginx 配置是否有误，成功后重启 Nginx 服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">nginx -t                <span class="hljs-comment"># 测试</span><br>nginx -s reload         <span class="hljs-comment"># 重新加载</span><br></code></pre></td></tr></table></figure><p><strong>注意</strong>，若修改了后端 Django 内容或其它内容，须重启 uWSGI 和 Nginx 服务，否则不生效！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">ps -ef | grep uwsgi         <span class="hljs-comment"># 查看uWSGI进程</span><br>killall -9 uwsgi            <span class="hljs-comment"># 用kill方法把uwsgi进程杀死</span><br>uwsgi --ini uwsgi.ini       <span class="hljs-comment"># 重启uwsgi</span><br>nginx -s reload             <span class="hljs-comment"># nginx平滑重启</span><br></code></pre></td></tr></table></figure><p>另外，如果你的项目文件存放于 root 用户目录下，访问网站时可能出现 500 或 403 Forbidden 权限报错，此时需修改 <code>/etc/nginx/nginx.conf</code>，将文件首行的 <code>user nginx</code> 修改为 <code>user root</code>。</p><p>至此网站已部署完毕，欢迎访问<a href="https://zewan.top/">我的网站</a>。项目<a href="https://github.com/ZewanHuang/Online-Publish-Vue">网上出版系统</a>已开源，欢迎交流学习！</p>]]></content>
    
    
    <categories>
      
      <category>配置与部署</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Web</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/06/16/notes/hello-world/"/>
    <url>/2021/06/16/notes/hello-world/</url>
    
    <content type="html"><![CDATA[<p>给博客换了个主题 Hexo Fluid，体验极佳，顺便简略记录一下部署过程和 Hexo 的一些基本用法。</p><span id="more"></span><h2 id="关于-Hexo"><a href="#关于-Hexo" class="headerlink" title="关于 Hexo"></a>关于 Hexo</h2><blockquote><p>引自官方文档</p></blockquote><p><a href="https://hexo.io/" target="_blank">Hexo</a> 是一个快速建议的博客框架，可使用 <a href="https://daringfireball.net/projects/markdown/" target="_blank">Markdown</a> 编辑页面。</p><p><a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank">Fluid</a> 是本博客采用的主题，十分美观，建议入坑。</p><h2 id="部署过程"><a href="#部署过程" class="headerlink" title="部署过程"></a>部署过程</h2><h3 id="1-准备环境-node-和-git"><a href="#1-准备环境-node-和-git" class="headerlink" title="1. 准备环境 node 和 git"></a>1. 准备环境 node 和 git</h3><p>node 环境的配置可以参照：<a href="https://www.cnblogs.com/jianguo221/p/11487532.html" target="_blank">windows安装npm教程–nodejs</a></p><p>上面参考资料只需要看前半部分。Git 安装较简单，大概下载安装即成功。</p><h3 id="2-安装-Hexo"><a href="#2-安装-Hexo" class="headerlink" title="2. 安装 Hexo"></a>2. 安装 Hexo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install -g hexo-cli<br></code></pre></td></tr></table></figure><p>安装完成后，找个你想放置博客项目文件的地方，执行下面命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init MyBlog<br><span class="hljs-built_in">cd</span> MyBlog<br>npm install<br></code></pre></td></tr></table></figure><p>新建完成后，文件夹目录与功能如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">.<br>├── _config.yml     <span class="hljs-comment"># 网站的配置信息，您可以在此配置大部分的参数。 </span><br>├── package.json<br>├── scaffolds       <span class="hljs-comment"># 模版文件夹</span><br>├── <span class="hljs-built_in">source</span>          <span class="hljs-comment"># 资源文件夹，除 _posts 文件，其他以下划线_开头的文件或者文件夹不会被编译打包到public文件夹</span><br>|   ├── _drafts     <span class="hljs-comment"># 草稿文件</span><br>|   └── _posts      <span class="hljs-comment"># 文章Markdowm文件 </span><br>└── themes          <span class="hljs-comment"># 主题文件夹</span><br></code></pre></td></tr></table></figure><h3 id="3-导入-fluid-主题"><a href="#3-导入-fluid-主题" class="headerlink" title="3. 导入 fluid 主题"></a>3. 导入 fluid 主题</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm install --save hexo-theme-fluid<br></code></pre></td></tr></table></figure><p>然后在博客目录下创建 _config.fluid.yml，该文件为主题配置文件，具体配置参见 <a href="https://hexo.fluid-dev.com/docs/guide/">Configuration</a>。我的个人配置仅作参考：<a href="https://paste.ubuntu.com/p/bmNjKsWZW9/" target="_blank">ZewanBlog_config.fluid.yml</a></p><p>如下修改 Hexo 博客目录中的 _config.yml：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">theme:</span> <span class="hljs-string">fluid</span>        <span class="hljs-comment"># 指定主题</span><br><span class="hljs-attr">language:</span> <span class="hljs-string">zh-CN</span>     <span class="hljs-comment"># 指定语言，会影响主题显示的语言，按需修改</span><br></code></pre></td></tr></table></figure><p>运行 <code>hexo s</code> 命令，在本地浏览 <a href="http://localhost:4000/">http://localhost:4000</a> 可以预览效果。</p><h3 id="4-新建-GitHub-仓库"><a href="#4-新建-GitHub-仓库" class="headerlink" title="4. 新建 GitHub 仓库"></a>4. 新建 GitHub 仓库</h3><p>注意仓库名必须是 用户名.github.io</p><h3 id="5-部署到-GitHub"><a href="#5-部署到-GitHub" class="headerlink" title="5. 部署到 GitHub"></a>5. 部署到 GitHub</h3><p>在 _config.yml 配置文件中修改：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repo:</span><br>    <span class="hljs-attr">github:</span> <span class="hljs-string">https://github.com/ZewanHuang/ZewanHuang.github.io</span>  <span class="hljs-comment"># 修改自己的仓库</span><br>  <span class="hljs-attr">branch:</span> <span class="hljs-string">master</span><br></code></pre></td></tr></table></figure><p>安装部署插件 <a href="https://github.com/hexojs/hexo-deployer-git" target="_blank">hexo-deployer-git</a>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-deployer-git --save<br></code></pre></td></tr></table></figure><p>最后执行命令部署上传，以下 g 是 generate 缩写，d 是 deploy 缩写：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean      <span class="hljs-comment"># 清除缓存文件和已生成的静态文件</span><br>hexo g -d       <span class="hljs-comment"># 部署上传</span><br></code></pre></td></tr></table></figure><h2 id="基础用法"><a href="#基础用法" class="headerlink" title="基础用法"></a>基础用法</h2><h3 id="创建关于页面"><a href="#创建关于页面" class="headerlink" title="创建关于页面"></a>创建关于页面</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page about     <span class="hljs-comment"># 创建关于页面</span><br></code></pre></td></tr></table></figure><h3 id="创建新页面"><a href="#创建新页面" class="headerlink" title="创建新页面"></a>创建新页面</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new <span class="hljs-string">&#x27;article-title&#x27;</span>    <span class="hljs-comment"># 新建页面</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="本地预览"><a href="#本地预览" class="headerlink" title="本地预览"></a>本地预览</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo s<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo g<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean                  <span class="hljs-comment"># 清除缓存</span><br>hexo g -d                   <span class="hljs-comment"># 部署上传</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
